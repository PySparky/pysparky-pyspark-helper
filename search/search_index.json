{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"PySparky - PySpark Helper","text":"<p>Welcome to PySparky, a set of helper functions designed to simplify your work with PySpark. This library provides utilities to make data transformation and analysis with PySpark easier and more efficient. - GitHub</p>"},{"location":"#introduction","title":"Introduction","text":"<p>PySparky is a collection of utility functions aimed at streamlining PySpark operations. Whether you're dealing with data transformation, cleaning, or analysis, PySparky offers helper functions that save you time and effort.</p> <p>It is designed to replicate the structure of PySpark, making it highly accessible for users.</p> <ul> <li>The <code>functions</code> folder contains all PySpark functions, where both the input and output are Columns.</li> <li>The <code>Spark_ext</code>  houses functions that necessitate a Spark instance, such as creating a DataFrame.</li> <li>The <code>transformation_ext</code>  includes functions associated with DataFrame transformations, where both the input and output are DataFrames</li> </ul>"},{"location":"#features","title":"Features","text":"<ul> <li>Easy Installation: Quickly integrate PySparky into your PySpark projects.</li> <li>Utility Functions: A wide range of helper functions for common PySpark tasks.</li> <li>Well-Documented: Clear and comprehensive documentation for all functions.</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>To install PySparky, simply download the whl from the repository:</p> <pre><code>pip install pysparky-x.y.z.whl\n# or\npip install git+https://github.com/PySparky/pysparky-pyspark-helper.git\n\n# remove\npip uninstall pysparky\n</code></pre>"},{"location":"#check-example-for-the-usage","title":"Check example for the usage","text":"<p>PySparky Example</p>"},{"location":"data_validator/","title":"Data validator","text":""},{"location":"data_validator/#pysparky.data_validator.DataValidator","title":"<code>DataValidator</code>  <code>dataclass</code>","text":"<p>A class to validate data based on a set of validation rules.</p> <p>Attributes:</p> Name Type Description <code>rules</code> <code>list[ValidationRule]</code> <p>A list of validation rules.</p> Example <pre><code>&gt;&gt;&gt; ValidationRules = [\n...     ValidationRule(\"first_name_check\", F_.is_printable_only(\"first_name\")),\n...     ValidationRule(\"last_name_check\", F_.is_printable_only(\"last_name\")),\n...     ValidationRule(\"address_check\", F_.is_printable_only(\"address\")),\n...     ValidationRule(\"region_check\", F_.is_printable_only(\"region\")),\n...     ValidationRule(\"code_check\", [F_.is_two_character_only(\"code\")]),\n...     ValidationRule(\"postcode_check\", F_.is_printable_only(\"postcode\")),\n... ]\n\n&gt;&gt;&gt; validator = DataValidator(ValidationRules)\n\n&gt;&gt;&gt; conditions = {\n...     \"first_name_check\": F_.is_printable_only(\"first_name\"),\n...     \"last_name_check\": F_.is_printable_only(\"last_name\"),\n...     \"address_check\": F_.is_printable_only(\"address\"),\n...     \"region_check\": F_.is_printable_only(\"region\"),\n...     \"code_check\": [F_.is_two_character_only(\"code\")],\n...     \"postcode_check\": F_.is_printable_only(\"postcode\"),\n... }\n\n&gt;&gt;&gt; validator = DataValidator.from_dict(conditions)\n</code></pre> Source code in <code>pysparky/data_validator.py</code> <pre><code>@dataclass\nclass DataValidator:\n    \"\"\"\n    A class to validate data based on a set of validation rules.\n\n    Attributes:\n        rules (list[ValidationRule]): A list of validation rules.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; ValidationRules = [\n        ...     ValidationRule(\"first_name_check\", F_.is_printable_only(\"first_name\")),\n        ...     ValidationRule(\"last_name_check\", F_.is_printable_only(\"last_name\")),\n        ...     ValidationRule(\"address_check\", F_.is_printable_only(\"address\")),\n        ...     ValidationRule(\"region_check\", F_.is_printable_only(\"region\")),\n        ...     ValidationRule(\"code_check\", [F_.is_two_character_only(\"code\")]),\n        ...     ValidationRule(\"postcode_check\", F_.is_printable_only(\"postcode\")),\n        ... ]\n\n        &gt;&gt;&gt; validator = DataValidator(ValidationRules)\n\n        &gt;&gt;&gt; conditions = {\n        ...     \"first_name_check\": F_.is_printable_only(\"first_name\"),\n        ...     \"last_name_check\": F_.is_printable_only(\"last_name\"),\n        ...     \"address_check\": F_.is_printable_only(\"address\"),\n        ...     \"region_check\": F_.is_printable_only(\"region\"),\n        ...     \"code_check\": [F_.is_two_character_only(\"code\")],\n        ...     \"postcode_check\": F_.is_printable_only(\"postcode\"),\n        ... }\n\n        &gt;&gt;&gt; validator = DataValidator.from_dict(conditions)\n        ```\n    \"\"\"\n\n    rules: list[ValidationRule]\n\n    @classmethod\n    def from_dict(cls, data: dict[str, Union[list[Column], Column]]) -&gt; \"DataValidator\":\n        \"\"\"\n        Creates a DataValidator instance from a dictionary.\n\n        Args:\n            data (dict[str, list[Column] | Column]): A dictionary where keys are rule names and values are lists of conditions or a single condition.\n\n        Returns:\n            DataValidator: An instance of DataValidator.\n\n        Example:\n            ```python\n            &gt;&gt;&gt; conditions = {\n            ...     \"first_name_check\": F_.is_printable_only(\"first_name\"),\n            ...     \"last_name_check\": F_.is_printable_only(\"last_name\"),\n            ...     \"address_check\": F_.is_printable_only(\"address\"),\n            ...     \"region_check\": F_.is_printable_only(\"region\"),\n            ...     \"code_check\": [F_.is_two_character_only(\"code\")],\n            ...     \"postcode_check\": F_.is_printable_only(\"postcode\"),\n            ... }\n            &gt;&gt;&gt; validator = DataValidator.from_dict(conditions)\n            ```\n\n        \"\"\"\n        rules = [ValidationRule(name, conditions) for name, conditions in data.items()]\n        return cls(rules=rules)\n\n    @property\n    def query_map(self) -&gt; dict[str, Column]:\n        \"\"\"\n        Gets a dictionary mapping rule names to their combined conditions.\n        The key is the column name and the value is the Column Object.\n\n        Returns:\n            dict[str, Column]: A dictionary where keys are rule names\n                and values are combined conditions.\n\n        Example:\n            ```python\n            &gt;&gt;&gt; sdf.withColumns(validator.query_map)\n            ```\n\n        \"\"\"\n        return {rule.name: rule.combined_condition for rule in self.rules}\n\n    def apply_conditions(self, sdf: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Applies the combined conditions to the Spark DataFrame.\n\n        Args:\n            sdf (DataFrame): The Spark DataFrame to which the conditions will be applied.\n\n        Returns:\n            DataFrame: The Spark DataFrame with the conditions applied.\n\n        Example:\n            ```python\n            &gt;&gt;&gt; validator.apply_conditions(data_sdf)\n            ```\n\n        \"\"\"\n        return sdf.withColumns(self.query_map)\n\n    def filter_invalid(self, sdf: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Filters out invalid rows from the Spark DataFrame based on the rules.\n\n        Args:\n            sdf (DataFrame): The Spark DataFrame to be filtered.\n\n        Returns:\n            DataFrame: The Spark DataFrame with invalid rows filtered out.\n\n        Example:\n            ```python\n            &gt;&gt;&gt; validator.filter_invalid(data_sdf)\n            ```\n\n        \"\"\"\n        return filters(\n            self.apply_conditions(sdf),\n            [\n                (\n                    F.col(column_name) == False  # noqa: E712\n                )  # pylint: disable=singleton-comparison\n                for column_name in self.query_map.keys()\n            ],\n            operator_=\"or\",\n        )\n\n    def filter_valid(self, sdf: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Filters out valid rows from the Spark DataFrame based on the rules.\n\n        Args:\n            sdf (DataFrame): The Spark DataFrame to be filtered.\n\n        Returns:\n            DataFrame: The Spark DataFrame with valid rows filtered out.\n\n        Example:\n            ```python\n            &gt;&gt;&gt; validator.filter_valid(data_sdf).select(data_sdf.columns).show()\n            ```\n\n        \"\"\"\n        return filters(\n            self.apply_conditions(sdf),\n            [\n                (\n                    F.col(column_name) == True  # noqa: E712\n                )  # noqa: E712 # pylint: disable=singleton-comparison\n                for column_name in self.query_map.keys()\n            ],\n            operator_=\"and\",\n        )\n</code></pre>"},{"location":"data_validator/#pysparky.data_validator.DataValidator.query_map","title":"<code>query_map</code>  <code>property</code>","text":"<p>Gets a dictionary mapping rule names to their combined conditions. The key is the column name and the value is the Column Object.</p> <p>Returns:</p> Type Description <code>dict[str, Column]</code> <p>dict[str, Column]: A dictionary where keys are rule names and values are combined conditions.</p> Example <pre><code>&gt;&gt;&gt; sdf.withColumns(validator.query_map)\n</code></pre>"},{"location":"data_validator/#pysparky.data_validator.DataValidator.apply_conditions","title":"<code>apply_conditions(sdf)</code>","text":"<p>Applies the combined conditions to the Spark DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>The Spark DataFrame to which the conditions will be applied.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>The Spark DataFrame with the conditions applied.</p> Example <pre><code>&gt;&gt;&gt; validator.apply_conditions(data_sdf)\n</code></pre> Source code in <code>pysparky/data_validator.py</code> <pre><code>def apply_conditions(self, sdf: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Applies the combined conditions to the Spark DataFrame.\n\n    Args:\n        sdf (DataFrame): The Spark DataFrame to which the conditions will be applied.\n\n    Returns:\n        DataFrame: The Spark DataFrame with the conditions applied.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; validator.apply_conditions(data_sdf)\n        ```\n\n    \"\"\"\n    return sdf.withColumns(self.query_map)\n</code></pre>"},{"location":"data_validator/#pysparky.data_validator.DataValidator.filter_invalid","title":"<code>filter_invalid(sdf)</code>","text":"<p>Filters out invalid rows from the Spark DataFrame based on the rules.</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>The Spark DataFrame to be filtered.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>The Spark DataFrame with invalid rows filtered out.</p> Example <pre><code>&gt;&gt;&gt; validator.filter_invalid(data_sdf)\n</code></pre> Source code in <code>pysparky/data_validator.py</code> <pre><code>def filter_invalid(self, sdf: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Filters out invalid rows from the Spark DataFrame based on the rules.\n\n    Args:\n        sdf (DataFrame): The Spark DataFrame to be filtered.\n\n    Returns:\n        DataFrame: The Spark DataFrame with invalid rows filtered out.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; validator.filter_invalid(data_sdf)\n        ```\n\n    \"\"\"\n    return filters(\n        self.apply_conditions(sdf),\n        [\n            (\n                F.col(column_name) == False  # noqa: E712\n            )  # pylint: disable=singleton-comparison\n            for column_name in self.query_map.keys()\n        ],\n        operator_=\"or\",\n    )\n</code></pre>"},{"location":"data_validator/#pysparky.data_validator.DataValidator.filter_valid","title":"<code>filter_valid(sdf)</code>","text":"<p>Filters out valid rows from the Spark DataFrame based on the rules.</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>The Spark DataFrame to be filtered.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>The Spark DataFrame with valid rows filtered out.</p> Example <pre><code>&gt;&gt;&gt; validator.filter_valid(data_sdf).select(data_sdf.columns).show()\n</code></pre> Source code in <code>pysparky/data_validator.py</code> <pre><code>def filter_valid(self, sdf: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Filters out valid rows from the Spark DataFrame based on the rules.\n\n    Args:\n        sdf (DataFrame): The Spark DataFrame to be filtered.\n\n    Returns:\n        DataFrame: The Spark DataFrame with valid rows filtered out.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; validator.filter_valid(data_sdf).select(data_sdf.columns).show()\n        ```\n\n    \"\"\"\n    return filters(\n        self.apply_conditions(sdf),\n        [\n            (\n                F.col(column_name) == True  # noqa: E712\n            )  # noqa: E712 # pylint: disable=singleton-comparison\n            for column_name in self.query_map.keys()\n        ],\n        operator_=\"and\",\n    )\n</code></pre>"},{"location":"data_validator/#pysparky.data_validator.DataValidator.from_dict","title":"<code>from_dict(data)</code>  <code>classmethod</code>","text":"<p>Creates a DataValidator instance from a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict[str, list[Column] | Column]</code> <p>A dictionary where keys are rule names and values are lists of conditions or a single condition.</p> required <p>Returns:</p> Name Type Description <code>DataValidator</code> <code>DataValidator</code> <p>An instance of DataValidator.</p> Example <pre><code>&gt;&gt;&gt; conditions = {\n...     \"first_name_check\": F_.is_printable_only(\"first_name\"),\n...     \"last_name_check\": F_.is_printable_only(\"last_name\"),\n...     \"address_check\": F_.is_printable_only(\"address\"),\n...     \"region_check\": F_.is_printable_only(\"region\"),\n...     \"code_check\": [F_.is_two_character_only(\"code\")],\n...     \"postcode_check\": F_.is_printable_only(\"postcode\"),\n... }\n&gt;&gt;&gt; validator = DataValidator.from_dict(conditions)\n</code></pre> Source code in <code>pysparky/data_validator.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: dict[str, Union[list[Column], Column]]) -&gt; \"DataValidator\":\n    \"\"\"\n    Creates a DataValidator instance from a dictionary.\n\n    Args:\n        data (dict[str, list[Column] | Column]): A dictionary where keys are rule names and values are lists of conditions or a single condition.\n\n    Returns:\n        DataValidator: An instance of DataValidator.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; conditions = {\n        ...     \"first_name_check\": F_.is_printable_only(\"first_name\"),\n        ...     \"last_name_check\": F_.is_printable_only(\"last_name\"),\n        ...     \"address_check\": F_.is_printable_only(\"address\"),\n        ...     \"region_check\": F_.is_printable_only(\"region\"),\n        ...     \"code_check\": [F_.is_two_character_only(\"code\")],\n        ...     \"postcode_check\": F_.is_printable_only(\"postcode\"),\n        ... }\n        &gt;&gt;&gt; validator = DataValidator.from_dict(conditions)\n        ```\n\n    \"\"\"\n    rules = [ValidationRule(name, conditions) for name, conditions in data.items()]\n    return cls(rules=rules)\n</code></pre>"},{"location":"data_validator/#pysparky.data_validator.ValidationRule","title":"<code>ValidationRule</code>  <code>dataclass</code>","text":"<p>A class to represent a validation rule.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The name of the validation rule.</p> <code>conditions</code> <code>list[Column]</code> <p>A list of conditions (Spark Columns) that make up the rule.</p> <code>combined_condition</code> <code>Column</code> <p>The combined condition of all the conditions using logical AND. It will generate from conditions</p> Example <p>ValidationRule(\"first_name_check\", F_.printable_only(\"first_name\")),</p> Source code in <code>pysparky/data_validator.py</code> <pre><code>@dataclass\nclass ValidationRule:\n    \"\"\"\n    A class to represent a validation rule.\n\n    Attributes:\n        name (str): The name of the validation rule.\n        conditions (list[Column]): A list of conditions (Spark Columns) that make up the rule.\n        combined_condition (Column): The combined condition of all the conditions using logical AND.\n            It will generate from conditions\n\n    Example:\n        ValidationRule(\"first_name_check\", F_.printable_only(\"first_name\")),\n    \"\"\"\n\n    name: str\n    conditions: list[Column] | Column\n    combined_condition: Column = field(init=False)\n\n    def __post_init__(self):\n        self.conditions = enabler.ensure_list(self.conditions)\n        self.combined_condition = F_.condition_and(*self.conditions)\n</code></pre>"},{"location":"debug/","title":"Debug","text":""},{"location":"debug/#pysparky.debug.get_distinct_value_from_df_columns","title":"<code>get_distinct_value_from_df_columns(df, columns_names, display=True)</code>","text":"<p>Get distinct values from specified DataFrame columns and optionally display their counts.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Spark DataFrame.</p> required <code>columns_names</code> <code>list[str]</code> <p>List of column names to process.</p> required <code>display</code> <code>bool</code> <p>Whether to display the counts of distinct values. Default is True.</p> <code>True</code> <p>Returns:</p> Type Description <code>dict[str, list]</code> <p>dict[str, list]: A dictionary where keys are column names and values are lists of distinct values.</p> Source code in <code>pysparky/debug.py</code> <pre><code>def get_distinct_value_from_df_columns(\n    df: DataFrame, columns_names: list[str], display: bool = True\n) -&gt; dict[str, list]:\n    \"\"\"\n    Get distinct values from specified DataFrame columns and optionally display their counts.\n\n    Args:\n        df (DataFrame): The input Spark DataFrame.\n        columns_names (list[str]): List of column names to process.\n        display (bool): Whether to display the counts of distinct values. Default is True.\n\n    Returns:\n        dict[str, list]: A dictionary where keys are column names and values are lists of distinct values.\n    \"\"\"\n    myDict = {}\n    for col in columns_names:\n        data = df.select(col).distinct()\n        myDict[col] = [row[col] for row in data.collect()]\n\n        if display:\n            if df.groupBy(col).count().count() &lt; 20:\n                df.groupBy(col).count().show()\n    return myDict\n</code></pre>"},{"location":"decorator/","title":"Decorator","text":""},{"location":"decorator/#pysparky.decorator.extension_enabler","title":"<code>extension_enabler(cls)</code>","text":"<p>This enable you to chain the class</p> Source code in <code>pysparky/decorator.py</code> <pre><code>def extension_enabler(cls):\n    \"\"\"\n    This enable you to chain the class\n    \"\"\"\n\n    def decorator(func):\n        # assign the function into the object\n        setattr(cls, f\"{func.__name__}\", func)\n\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            func_return = func(*args, **kwargs)\n            return func_return\n\n        return wrapper\n\n    return decorator\n</code></pre>"},{"location":"enabler/","title":"Enabler","text":""},{"location":"enabler/#pysparky.enabler.ensure_column","title":"<code>ensure_column(*columns)</code>","text":"<p>Enables PySpark functions to accept either column names (as strings) or Column objects.</p> <p>Parameters: columns (ColumnOrName): Column names (as strings) or Column objects to be converted.</p> <p>Returns: tuple[Column]: A tuple of Column objects.</p> <p>Example: <pre><code>&gt;&gt;&gt; ensure_column(\"col1\", \"col2\", F.col(\"col3\"))\n(Column&lt;b'col1'&gt;, Column&lt;b'col2'&gt;, Column&lt;b'col3'&gt;)\n</code></pre></p> Source code in <code>pysparky/enabler.py</code> <pre><code>def ensure_column(*columns: ColumnOrName) -&gt; tuple[Column, ...]:\n    \"\"\"\n    Enables PySpark functions to accept either column names (as strings) or Column objects.\n\n    Parameters:\n    columns (ColumnOrName): Column names (as strings) or Column objects to be converted.\n\n    Returns:\n    tuple[Column]: A tuple of Column objects.\n\n    Example:\n    ``` py\n    &gt;&gt;&gt; ensure_column(\"col1\", \"col2\", F.col(\"col3\"))\n    (Column&lt;b'col1'&gt;, Column&lt;b'col2'&gt;, Column&lt;b'col3'&gt;)\n    ```\n\n    \"\"\"\n    return tuple(\n        map(\n            lambda column: F.col(column) if isinstance(column, str) else column, columns\n        )\n    )\n</code></pre>"},{"location":"enabler/#pysparky.enabler.ensure_list","title":"<code>ensure_list(single_or_list)</code>","text":"<p>Ensures the input is returned as a list.</p> <p>If the input is not already a list, it wraps the input in a list. If the input is already a list, it returns the input unchanged.</p> <p>Parameters:</p> Name Type Description Default <code>single_or_list</code> <code>Union[Any, List[Any]]</code> <p>The input which can be a single item or a list of items.</p> required <p>Returns:</p> Type Description <code>list[Any]</code> <p>List[Any]: A list containing the input item(s).</p> Example <pre><code>&gt;&gt;&gt; ensure_list(5)\n\n&gt;&gt;&gt; ensure_list([1, 2, 3])\n[1, 2, 3]\n&gt;&gt;&gt; ensure_list(\"hello\")\n[\"hello\"]\n</code></pre> Source code in <code>pysparky/enabler.py</code> <pre><code>def ensure_list(single_or_list: Any | list[Any]) -&gt; list[Any]:\n    \"\"\"\n    Ensures the input is returned as a list.\n\n    If the input is not already a list, it wraps the input in a list.\n    If the input is already a list, it returns the input unchanged.\n\n    Args:\n        single_or_list (Union[Any, List[Any]]): The input which can be a single item or a list of items.\n\n    Returns:\n        List[Any]: A list containing the input item(s).\n\n    Example:\n        ``` py\n        &gt;&gt;&gt; ensure_list(5)\n\n        &gt;&gt;&gt; ensure_list([1, 2, 3])\n        [1, 2, 3]\n        &gt;&gt;&gt; ensure_list(\"hello\")\n        [\"hello\"]\n        ```\n\n    \"\"\"\n    return single_or_list if isinstance(single_or_list, list) else [single_or_list]\n</code></pre>"},{"location":"quality/","title":"Quality","text":""},{"location":"quality/#pysparky.quality.expect_any_to_one","title":"<code>expect_any_to_one(col1, col2)</code>","text":"<p>A decorator function that ensures an N:1 relationship between col1 and col2, meaning each value in col1 corresponds to only one distinct value in col2.</p> <p>Parameters:</p> Name Type Description Default <code>col1</code> <code>str | Sequence[str]</code> <p>Name of the column or a tuple of column names.</p> required <code>col2</code> <code>str | Sequence[str]</code> <p>Name of the column or a tuple of column names.</p> required Example <pre><code>&gt;&gt;&gt; @expect_any_to_one(\"city\", \"country\")\n... def get_data():\n...     return spark.createDataFrame([(\"Paris\", \"France\"), (\"Lyon\", \"France\")], [\"city\", \"country\"])\n&gt;&gt;&gt; df = get_data()\n\u2705: city:country is N:1\n</code></pre> Source code in <code>pysparky/quality.py</code> <pre><code>def expect_any_to_one(col1: str | Sequence[str], col2: str | Sequence[str]):\n    \"\"\"\n    A decorator function that ensures an N:1 relationship between col1 and col2,\n    meaning each value in col1 corresponds to only one distinct value in col2.\n\n    Args:\n        col1 (str | Sequence[str]): Name of the column or a tuple of column names.\n        col2 (str | Sequence[str]): Name of the column or a tuple of column names.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; @expect_any_to_one(\"city\", \"country\")\n        ... def get_data():\n        ...     return spark.createDataFrame([(\"Paris\", \"France\"), (\"Lyon\", \"France\")], [\"city\", \"country\"])\n        &gt;&gt;&gt; df = get_data()\n        \u2705: city:country is N:1\n        ```\n    \"\"\"\n\n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            spark_table_sdf = func(*args, **kwargs)\n            num_col2_values_with_many_col1_values = (\n                spark_table_sdf.groupBy(*col1)\n                .agg(F.count_distinct(*col2).alias(\"distinct_count\"))\n                .where(F.col(\"distinct_count\") &gt; 1)\n                .count()\n            )\n            assert (\n                num_col2_values_with_many_col1_values == 0\n            ), f\"Multiple {col2}s per {col1}\"\n            print(f\"\u2705: {col1}:{col2} is N:1\")\n            return spark_table_sdf\n\n        return wrapper\n\n    return decorator\n</code></pre>"},{"location":"quality/#pysparky.quality.expect_criteria","title":"<code>expect_criteria(criteria)</code>","text":"<p>A decorator function that ensures a specific criterion on a Spark DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>criteria</code> <code>Column</code> <p>The filter criterion to be applied to the DataFrame.</p> required <p>Returns:</p> Name Type Description <code>function</code> <p>A decorated function that checks the criterion.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the filtered count and unfiltered count of the DataFrame are not equal.</p> Example <pre><code>&gt;&gt;&gt; @expect_criteria(F.col(\"age\") &gt; 0)\n... def get_data():\n...     return spark.createDataFrame([(10,), (20,)], [\"age\"])\n&gt;&gt;&gt; df = get_data()\n\u2705: Criteria '(age &gt; 0)' passed\n</code></pre> Source code in <code>pysparky/quality.py</code> <pre><code>def expect_criteria(criteria):\n    \"\"\"\n    A decorator function that ensures a specific criterion on a Spark DataFrame.\n\n    Args:\n        criteria (pyspark.sql.column.Column): The filter criterion to be applied to the DataFrame.\n\n    Returns:\n        function: A decorated function that checks the criterion.\n\n    Raises:\n        AssertionError: If the filtered count and unfiltered count of the DataFrame are not equal.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; @expect_criteria(F.col(\"age\") &gt; 0)\n        ... def get_data():\n        ...     return spark.createDataFrame([(10,), (20,)], [\"age\"])\n        &gt;&gt;&gt; df = get_data()\n        \u2705: Criteria '(age &gt; 0)' passed\n        ```\n    \"\"\"\n\n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            spark_table_sdf = func(*args, **kwargs)\n            filtered_count = spark_table_sdf.filter(criteria).count()\n            unfiltered_count = spark_table_sdf.count()\n            assert (\n                filtered_count == unfiltered_count\n            ), f\"Filtered count is not equal to unfiltered count {criteria}\"\n            print(f\"\u2705: Criteria '{criteria}' passed\")\n            return spark_table_sdf\n\n        return wrapper\n\n    return decorator\n</code></pre>"},{"location":"quality/#pysparky.quality.expect_one_to_one","title":"<code>expect_one_to_one(col1, col2)</code>","text":"<p>A decorator function that ensures a 1:1 relationship between col1 and col2, meaning each value in col1 corresponds to only one distinct value in col2 and vice-versa.</p> <p>Parameters:</p> Name Type Description Default <code>col1</code> <code>str | Sequence[str]</code> <p>Name of the column or a tuple of column names.</p> required <code>col2</code> <code>str | Sequence[str]</code> <p>Name of the column or a tuple of column names.</p> required Example <pre><code>&gt;&gt;&gt; @expect_one_to_one(\"id\", \"name\")\n... def get_data():\n...     return spark.createDataFrame([(1, \"Alice\"), (2, \"Bob\")], [\"id\", \"name\"])\n&gt;&gt;&gt; df = get_data()\n\u2705: id:name is N:1\n\u2705: name:id is N:1\n</code></pre> Source code in <code>pysparky/quality.py</code> <pre><code>def expect_one_to_one(col1: str | Sequence[str], col2: str | Sequence[str]):\n    \"\"\"\n    A decorator function that ensures a 1:1 relationship between col1 and col2,\n    meaning each value in col1 corresponds to only one distinct value in col2 and vice-versa.\n\n    Args:\n        col1 (str | Sequence[str]): Name of the column or a tuple of column names.\n        col2 (str | Sequence[str]): Name of the column or a tuple of column names.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; @expect_one_to_one(\"id\", \"name\")\n        ... def get_data():\n        ...     return spark.createDataFrame([(1, \"Alice\"), (2, \"Bob\")], [\"id\", \"name\"])\n        &gt;&gt;&gt; df = get_data()\n        \u2705: id:name is N:1\n        \u2705: name:id is N:1\n        ```\n    \"\"\"\n\n    any_to_one = expect_any_to_one(col1, col2)\n    one_to_any = expect_any_to_one(col2, col1)\n\n    def decorator(func):\n        return one_to_any(any_to_one(func))\n\n    return decorator\n</code></pre>"},{"location":"quality/#pysparky.quality.expect_type","title":"<code>expect_type(col_name, col_type)</code>","text":"<p>A decorator function that verifies the data type of a specified column in a Spark DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>col_name</code> <code>str</code> <p>The column's name.</p> required <code>col_type</code> <code>DataType</code> <p>The expected data type for the column.</p> required <p>Returns:</p> Name Type Description <code>function</code> <p>A decorated function that checks the column's data type.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the column's data type does not match the expected type.</p> Example <pre><code>&gt;&gt;&gt; from pyspark.sql.types import IntegerType\n&gt;&gt;&gt; @expect_type(\"id\", IntegerType())\n... def get_data():\n...     return spark.createDataFrame([(1,), (2,)], [\"id\"])\n&gt;&gt;&gt; df = get_data()\n\u2705: Column 'id' has the expected data type IntegerType()\n</code></pre> Source code in <code>pysparky/quality.py</code> <pre><code>def expect_type(col_name, col_type):\n    \"\"\"\n    A decorator function that verifies the data type of a specified column in a Spark DataFrame.\n\n    Args:\n        col_name (str): The column's name.\n        col_type (pyspark.sql.types.DataType): The expected data type for the column.\n\n    Returns:\n        function: A decorated function that checks the column's data type.\n\n    Raises:\n        AssertionError: If the column's data type does not match the expected type.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; from pyspark.sql.types import IntegerType\n        &gt;&gt;&gt; @expect_type(\"id\", IntegerType())\n        ... def get_data():\n        ...     return spark.createDataFrame([(1,), (2,)], [\"id\"])\n        &gt;&gt;&gt; df = get_data()\n        \u2705: Column 'id' has the expected data type IntegerType()\n        ```\n    \"\"\"\n\n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            spark_table_sdf = func(*args, **kwargs)\n            source_type = spark_table_sdf.schema[col_name].dataType\n            target_type = col_type\n            assert (\n                source_type == target_type\n            ), f\"Data type of column '{col_name}:{source_type}' is not equal to {target_type}\"  # noqa: E501\n            print(f\"\u2705: Column '{col_name}' has the expected data type {col_type}\")\n            return spark_table_sdf\n\n        return wrapper\n\n    return decorator\n</code></pre>"},{"location":"quality/#pysparky.quality.expect_unique","title":"<code>expect_unique(col_name)</code>","text":"<p>A decorator function that ensures the uniqueness of a column in a Spark DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>col_name</code> <code>str</code> <p>The column's name.</p> required <p>Returns:</p> Name Type Description <code>function</code> <p>A decorated function that checks the column's uniqueness.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the column's count and distinct count are not equal.</p> Example <pre><code>&gt;&gt;&gt; @expect_unique(\"id\")\n... def get_data():\n...     return spark.createDataFrame([(1,), (2,)], [\"id\"])\n&gt;&gt;&gt; df = get_data()\n\u2705: Column 'id' is distinct\n</code></pre> Source code in <code>pysparky/quality.py</code> <pre><code>def expect_unique(col_name):\n    \"\"\"\n    A decorator function that ensures the uniqueness of a column in a Spark DataFrame.\n\n    Args:\n        col_name (str): The column's name.\n\n    Returns:\n        function: A decorated function that checks the column's uniqueness.\n\n    Raises:\n        AssertionError: If the column's count and distinct count are not equal.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; @expect_unique(\"id\")\n        ... def get_data():\n        ...     return spark.createDataFrame([(1,), (2,)], [\"id\"])\n        &gt;&gt;&gt; df = get_data()\n        \u2705: Column 'id' is distinct\n        ```\n    \"\"\"\n\n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            spark_table_sdf = func(*args, **kwargs)\n            spark_table_col_sdf = spark_table_sdf.select(col_name)\n            normal_count = spark_table_col_sdf.count()\n            distinct_count = spark_table_col_sdf.distinct().count()\n            assert (\n                normal_count == distinct_count\n            ), f\"Count and distinct count of column '{col_name}' are not equal\"\n            print(f\"\u2705: Column '{col_name}' is distinct\")\n            return spark_table_sdf\n\n        return wrapper\n\n    return decorator\n</code></pre>"},{"location":"schema_ext/","title":"Schema ext","text":""},{"location":"schema_ext/#pysparky.schema_ext.filter_columns_by_datatype","title":"<code>filter_columns_by_datatype(struct_type, data_type)</code>","text":"<p>Filters and returns a StructType of StructField names from a given StructType schema that match the specified data type.</p> <p>Parameters:</p> Name Type Description Default <code>struct_type</code> <code>StructType</code> <p>The schema of the DataFrame.</p> required <code>data_type</code> <code>DataType</code> <p>The data type to filter by.</p> required <p>Returns:</p> Type Description <code>StructType</code> <p>T.StructType: A StructType of StructField names that match the specified data type.</p> Example <pre><code>&gt;&gt;&gt; schema = T.StructType([\n...     T.StructField(\"id\", T.IntegerType(), True),\n...     T.StructField(\"name\", T.StringType(), True),\n...     T.StructField(\"age\", T.IntegerType(), True)\n... ])\n&gt;&gt;&gt; filtered_schema = filter_columns_by_datatype(schema, T.IntegerType())\n&gt;&gt;&gt; print(filtered_schema)\nStructType([StructField('id', IntegerType(), True), StructField('age', IntegerType(), True)])\n</code></pre> Source code in <code>pysparky/schema_ext.py</code> <pre><code>@decorator.extension_enabler(T.StructType)\ndef filter_columns_by_datatype(\n    struct_type: T.StructType, data_type: T.DataType\n) -&gt; T.StructType:\n    \"\"\"\n    Filters and returns a StructType of StructField names from a given StructType schema\n    that match the specified data type.\n\n    Args:\n        struct_type (T.StructType): The schema of the DataFrame.\n        data_type (T.DataType): The data type to filter by.\n\n    Returns:\n        T.StructType: A StructType of StructField names that match the specified data type.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; schema = T.StructType([\n        ...     T.StructField(\"id\", T.IntegerType(), True),\n        ...     T.StructField(\"name\", T.StringType(), True),\n        ...     T.StructField(\"age\", T.IntegerType(), True)\n        ... ])\n        &gt;&gt;&gt; filtered_schema = filter_columns_by_datatype(schema, T.IntegerType())\n        &gt;&gt;&gt; print(filtered_schema)\n        StructType([StructField('id', IntegerType(), True), StructField('age', IntegerType(), True)])\n        ```\n    \"\"\"\n    return T.StructType([field for field in struct_type if field.dataType == data_type])\n</code></pre>"},{"location":"spark_ext/","title":"Spark ext","text":""},{"location":"spark_ext/#pysparky.spark_ext.check_table_exists","title":"<code>check_table_exists(spark, catalog, database, table_name)</code>","text":"<p>Checks if a specific table exists in the given catalog.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>The Spark session.</p> required <code>catalog</code> <code>str</code> <p>The catalog name.</p> required <code>table_name</code> <code>str</code> <p>The name of the table to check.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the table exists, False otherwise.</p> Source code in <code>pysparky/spark_ext.py</code> <pre><code>def check_table_exists(\n    spark: SparkSession, catalog: str, database: str, table_name: str\n) -&gt; bool:\n    \"\"\"\n    Checks if a specific table exists in the given catalog.\n\n    Args:\n        spark (SparkSession): The Spark session.\n        catalog (str): The catalog name.\n        table_name (str): The name of the table to check.\n\n    Returns:\n        bool: True if the table exists, False otherwise.\n    \"\"\"\n    # Retrieve the list of tables in the specified catalog\n    tables = spark.sql(f\"SHOW TABLES IN {catalog}.{database}\").collect()\n\n    # Check if the specific table exists\n    table_exists = any(table.tableName == table_name for table in tables)\n\n    return table_exists\n</code></pre>"},{"location":"spark_ext/#pysparky.spark_ext.column_function","title":"<code>column_function(spark, column_obj)</code>","text":"<p>Evaluates a Column expression in the context of a single-row DataFrame.</p> <p>This function creates a DataFrame with a single row and applies the given Column expression to it. This is particularly useful for testing Column expressions, evaluating complex transformations, or creating sample data based on Column operations.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>The SparkSession object.</p> required <code>column_obj</code> <code>Column</code> <p>The Column object or expression to evaluate.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.DataFrame: A single-row DataFrame containing the result of the Column expression.</p> Example <pre><code>from pyspark.sql import SparkSession, functions as F\nspark = SparkSession.builder.getOrCreate()\n</code></pre> Notes <ul> <li>This function is particularly useful for debugging or testing Column expressions   without the need to create a full DataFrame.</li> <li>The resulting DataFrame will always have a single column named 'col0' unless   the input Column object has a specific alias.</li> <li>Be cautious when using this with resource-intensive operations, as it still   creates a distributed DataFrame operation.</li> </ul> Source code in <code>pysparky/spark_ext.py</code> <pre><code>@decorator.extension_enabler(SparkSession)\ndef column_function(spark: SparkSession, column_obj: Column) -&gt; DataFrame:\n    \"\"\"\n    Evaluates a Column expression in the context of a single-row DataFrame.\n\n    This function creates a DataFrame with a single row and applies the given Column\n    expression to it. This is particularly useful for testing Column expressions,\n    evaluating complex transformations, or creating sample data based on Column operations.\n\n    Args:\n        spark (pyspark.sql.SparkSession): The SparkSession object.\n        column_obj (pyspark.sql.Column): The Column object or expression to evaluate.\n\n    Returns:\n        pyspark.sql.DataFrame: A single-row DataFrame containing the result of the Column expression.\n\n    Example:\n        ``` py\n        from pyspark.sql import SparkSession, functions as F\n        spark = SparkSession.builder.getOrCreate()\n        ```\n        # Simple column expression\n        ``` py\n        result = spark.column_function(F.lit(\"Hello, World!\"))\n        result.show()\n        +-------------+\n        |         col0|\n        +-------------+\n        |Hello, World!|\n        +-------------+\n        ```\n\n        # Complex column expression\n        ``` py\n        import datetime\n        complex_col = F.when(F.current_date() &gt; F.lit(datetime.date(2023, 1, 1)), \"Future\")\n        ...                .otherwise(\"Past\")\n        result = spark.column_function(complex_col)\n        result.show()\n        +------+\n        |  col0|\n        +------+\n        |Future|\n        +------+\n        ```\n\n        # Using with user-defined functions (UDFs)\n        ``` py\n        from pyspark.sql.types import IntegerType\n        square_udf = F.udf(lambda x: x * x, IntegerType())\n        result = spark.column_function(square_udf(F.lit(5)))\n        result.show()\n        +----+\n        |col0|\n        +----+\n        |  25|\n        +----+\n        ```\n\n    Notes:\n        - This function is particularly useful for debugging or testing Column expressions\n          without the need to create a full DataFrame.\n        - The resulting DataFrame will always have a single column named 'col0' unless\n          the input Column object has a specific alias.\n        - Be cautious when using this with resource-intensive operations, as it still\n          creates a distributed DataFrame operation.\n    \"\"\"\n    return spark.range(1).select(column_obj)\n</code></pre>"},{"location":"spark_ext/#pysparky.spark_ext.column_function--simple-column-expression","title":"Simple column expression","text":"<pre><code>result = spark.column_function(F.lit(\"Hello, World!\"))\nresult.show()\n+-------------+\n|         col0|\n+-------------+\n|Hello, World!|\n+-------------+\n</code></pre>"},{"location":"spark_ext/#pysparky.spark_ext.column_function--complex-column-expression","title":"Complex column expression","text":"<pre><code>import datetime\ncomplex_col = F.when(F.current_date() &gt; F.lit(datetime.date(2023, 1, 1)), \"Future\")\n...                .otherwise(\"Past\")\nresult = spark.column_function(complex_col)\nresult.show()\n+------+\n|  col0|\n+------+\n|Future|\n+------+\n</code></pre>"},{"location":"spark_ext/#pysparky.spark_ext.column_function--using-with-user-defined-functions-udfs","title":"Using with user-defined functions (UDFs)","text":"<pre><code>from pyspark.sql.types import IntegerType\nsquare_udf = F.udf(lambda x: x * x, IntegerType())\nresult = spark.column_function(square_udf(F.lit(5)))\nresult.show()\n+----+\n|col0|\n+----+\n|  25|\n+----+\n</code></pre>"},{"location":"spark_ext/#pysparky.spark_ext.convert_1d_list_to_dataframe","title":"<code>convert_1d_list_to_dataframe(spark, list_, column_names, axis='column')</code>","text":"<p>Converts a 1-dimensional list into a PySpark DataFrame.</p> <p>This function takes a 1-dimensional list and converts it into a PySpark DataFrame with the specified column names. The list can be converted into a DataFrame with either a single column or a single row, based on the specified axis.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>The Spark session to use for creating the DataFrame.</p> required <code>list_</code> <code>list</code> <p>The 1-dimensional list to convert.</p> required <code>column_names</code> <code>str or list of str</code> <p>The name(s) of the column(s) for the DataFrame.</p> required <code>axis</code> <code>str</code> <p>Specifies whether to convert the list into a single column or a single row.         Acceptable values are \"column\" (default) and \"row\".</p> <code>'column'</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A PySpark DataFrame created from the 1-dimensional list.</p> <p>Raises:</p> Type Description <code>AttributeError</code> <p>If the axis parameter is not \"column\" or \"row\".</p> Example <pre><code>&gt;&gt;&gt; spark = SparkSession.builder.appName(\"example\").getOrCreate()\n&gt;&gt;&gt; list_ = [1, 2, 3, 4]\n&gt;&gt;&gt; column_names = [\"numbers\"]\n&gt;&gt;&gt; df = convert_1d_list_to_dataframe(spark, list_, column_names, axis=\"column\")\n&gt;&gt;&gt; df.show()\n+-------+\n|numbers|\n+-------+\n|      1|\n|      2|\n|      3|\n|      4|\n+-------+\n&gt;&gt;&gt; column_names = [\"ID1\", \"ID2\", \"ID3\", \"ID4\"]\n&gt;&gt;&gt; df = convert_1d_list_to_dataframe(spark, list_, column_names, axis=\"row\")\n&gt;&gt;&gt; df.show()\n+---+---+---+---+\n|ID1|ID2|ID3|ID4|\n+---+---+---+---+\n|  1|  2|  3|  4|\n+---+---+---+---+\n</code></pre> Source code in <code>pysparky/spark_ext.py</code> <pre><code>@decorator.extension_enabler(SparkSession)\n# @decorator.column_name_or_column_names_enabler(\"column_names\")\ndef convert_1d_list_to_dataframe(\n    spark: SparkSession,\n    list_: list[Any],\n    column_names: str | list[str],\n    axis: str = \"column\",\n) -&gt; DataFrame:\n    \"\"\"\n    Converts a 1-dimensional list into a PySpark DataFrame.\n\n    This function takes a 1-dimensional list and converts it into a PySpark DataFrame\n    with the specified column names. The list can be converted into a DataFrame with\n    either a single column or a single row, based on the specified axis.\n\n    Parameters:\n        spark (SparkSession): The Spark session to use for creating the DataFrame.\n        list_ (list): The 1-dimensional list to convert.\n        column_names (str or list of str): The name(s) of the column(s) for the DataFrame.\n        axis (str): Specifies whether to convert the list into a single column or a single row.\n                    Acceptable values are \"column\" (default) and \"row\".\n\n    Returns:\n        DataFrame: A PySpark DataFrame created from the 1-dimensional list.\n\n    Raises:\n        AttributeError: If the axis parameter is not \"column\" or \"row\".\n\n    Example:\n        ``` py\n        &gt;&gt;&gt; spark = SparkSession.builder.appName(\"example\").getOrCreate()\n        &gt;&gt;&gt; list_ = [1, 2, 3, 4]\n        &gt;&gt;&gt; column_names = [\"numbers\"]\n        &gt;&gt;&gt; df = convert_1d_list_to_dataframe(spark, list_, column_names, axis=\"column\")\n        &gt;&gt;&gt; df.show()\n        +-------+\n        |numbers|\n        +-------+\n        |      1|\n        |      2|\n        |      3|\n        |      4|\n        +-------+\n        &gt;&gt;&gt; column_names = [\"ID1\", \"ID2\", \"ID3\", \"ID4\"]\n        &gt;&gt;&gt; df = convert_1d_list_to_dataframe(spark, list_, column_names, axis=\"row\")\n        &gt;&gt;&gt; df.show()\n        +---+---+---+---+\n        |ID1|ID2|ID3|ID4|\n        +---+---+---+---+\n        |  1|  2|  3|  4|\n        +---+---+---+---+\n        ```\n    \"\"\"\n    column_names = enabler.ensure_list(column_names)\n\n    if axis not in [\"column\", \"row\"]:\n        raise AttributeError(\n            f\"Invalid axis value: {axis}. Acceptable values are 'column' or 'row'.\"\n        )\n\n    if axis == \"column\":\n        tuple_list = ((x,) for x in list_)  # type: ignore\n    elif axis == \"row\":\n        tuple_list = (tuple(list_),)  # type: ignore\n\n    output_sdf = spark.createDataFrame(tuple_list, schema=column_names)\n\n    return output_sdf\n</code></pre>"},{"location":"spark_ext/#pysparky.spark_ext.convert_dict_to_dataframe","title":"<code>convert_dict_to_dataframe(spark, dict_, column_names, explode=False)</code>","text":"<p>Transforms a dictionary with list values into a Spark DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>dict_</code> <code>dict</code> <p>The dictionary to transform. Keys will become the first column, and values will become the second column.</p> required <code>column_names</code> <code>list[str]</code> <p>A list containing the names of the columns.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.DataFrame: A DataFrame with the dictionary keys and their corresponding exploded list values.</p> Example <pre><code>datadict_ = {\n    \"key1\": 1,\n    \"key2\": 2\n}\ncolumn_names = [\"keys\", \"values\"]\ndf = convert_dict_to_dataframe(datadict_, column_names)\ndisplay(df)\n# key1,1\n# key2,2\n</code></pre> Source code in <code>pysparky/spark_ext.py</code> <pre><code>@decorator.extension_enabler(SparkSession)\ndef convert_dict_to_dataframe(\n    spark: SparkSession,\n    dict_: dict[str, Any],\n    column_names: list[str],\n    explode: bool = False,\n) -&gt; DataFrame:\n    \"\"\"\n    Transforms a dictionary with list values into a Spark DataFrame.\n\n    Args:\n        dict_ (dict): The dictionary to transform. Keys will become the first column, and values will become the second column.\n        column_names (list[str]): A list containing the names of the columns.\n\n    Returns:\n        pyspark.sql.DataFrame: A DataFrame with the dictionary keys and their corresponding exploded list values.\n\n    Example:\n        ``` py\n        datadict_ = {\n            \"key1\": 1,\n            \"key2\": 2\n        }\n        column_names = [\"keys\", \"values\"]\n        df = convert_dict_to_dataframe(datadict_, column_names)\n        display(df)\n        # key1,1\n        # key2,2\n        ```\n\n    \"\"\"\n\n    # Assert that the input dictionary is not empty and column_names has exactly two elements\n    assert isinstance(dict_, dict), \"Input must be a dictionary\"\n    assert len(column_names) == 2, \"Column names list must contain exactly two elements\"\n\n    output_sdf = spark.createDataFrame(dict_.items(), column_names)\n\n    if explode:\n        assert all(\n            isinstance(val, list) for val in dict_.values()\n        ), \"All values in the dictionary must be lists\"\n        output_sdf = output_sdf.withColumn(column_names[1], F.explode(column_names[1]))\n\n    return output_sdf\n</code></pre>"},{"location":"spark_ext/#pysparky.spark_ext.createDataFrame_from_dict","title":"<code>createDataFrame_from_dict(spark, dict_)</code>","text":"<p>Creates a Spark DataFrame from a dictionary in a pandas-like style.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>The SparkSession object.</p> required <code>dict_</code> <code>dict</code> <p>The dictionary to convert, where keys are column names and values are lists of column data.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>The resulting Spark DataFrame.</p> Source code in <code>pysparky/spark_ext.py</code> <pre><code>@decorator.extension_enabler(SparkSession)\ndef createDataFrame_from_dict(spark: SparkSession, dict_: dict) -&gt; DataFrame:\n    \"\"\"\n    Creates a Spark DataFrame from a dictionary in a pandas-like style.\n\n    Args:\n        spark: The SparkSession object.\n        dict_ (dict): The dictionary to convert, where keys are column names and values are lists of column data.\n\n    Returns:\n        DataFrame: The resulting Spark DataFrame.\n    \"\"\"\n    data = list(zip(*dict_.values()))\n    label = list(dict_.keys())\n    return spark.createDataFrame(data, label)\n</code></pre>"},{"location":"test_admonition/","title":"Test admonition","text":"<p>Test</p> <p>This is a test.</p>"},{"location":"utils/","title":"Utils","text":""},{"location":"utils/#pysparky.utils.create_map_from_dict","title":"<code>create_map_from_dict(dict_)</code>","text":"<p>Generates a PySpark map column from a provided dictionary.</p> <p>This function converts a dictionary into a PySpark map column, with each key-value pair represented as a literal in the map.</p> <p>Parameters:</p> Name Type Description Default <code>dict_</code> <code>Dict[str, int]</code> <p>A dictionary with string keys and integer values.</p> required <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>A PySpark Column object representing the created map.</p> Example <p>dict_ = {\"a\": 1, \"b\": 2} map_column = create_map_from_dict(dict_)</p> Source code in <code>pysparky/utils.py</code> <pre><code>def create_map_from_dict(dict_: dict[str, int]) -&gt; Column:\n    \"\"\"\n    Generates a PySpark map column from a provided dictionary.\n\n    This function converts a dictionary into a PySpark map column, with each key-value pair represented as a literal in the map.\n\n    Parameters:\n        dict_ (Dict[str, int]): A dictionary with string keys and integer values.\n\n    Returns:\n        Column: A PySpark Column object representing the created map.\n\n    Example:\n        &gt;&gt;&gt; dict_ = {\"a\": 1, \"b\": 2}\n        &gt;&gt;&gt; map_column = create_map_from_dict(dict_)\n    \"\"\"\n\n    return F.create_map(list(map(F.lit, itertools.chain(*dict_.items()))))\n</code></pre>"},{"location":"utils/#pysparky.utils.join_dataframes_on_column","title":"<code>join_dataframes_on_column(column_name, *dataframes, how='outer')</code>","text":"<p>Joins a list of DataFrames on a specified column using an outer join.</p> <p>Parameters:</p> Name Type Description Default <code>column_name</code> <code>str</code> <p>The column name to join on.</p> required <code>*dataframes</code> <code>DataFrame</code> <p>A list of DataFrames to join.</p> <code>()</code> <code>how</code> <code>str</code> <p>The type of join to perform, passthrough to pyspark join (default is \"outer\").</p> <code>'outer'</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>The resulting DataFrame after performing the outer joins.</p> Source code in <code>pysparky/utils.py</code> <pre><code>def join_dataframes_on_column(\n    column_name: str, *dataframes: DataFrame, how: str = \"outer\"\n) -&gt; DataFrame:\n    \"\"\"\n    Joins a list of DataFrames on a specified column using an outer join.\n\n    Args:\n        column_name (str): The column name to join on.\n        *dataframes (DataFrame): A list of DataFrames to join.\n        how (str): The type of join to perform, passthrough to pyspark join (default is \"outer\").\n\n    Returns:\n        DataFrame: The resulting DataFrame after performing the outer joins.\n    \"\"\"\n\n    if not dataframes:\n        raise ValueError(\"At least one DataFrame must be provided\")\n\n    # Check if all DataFrames have the specified column\n    if not all(column_name in df.columns for df in dataframes):\n        raise ValueError(f\"Column '{column_name}' not found in all DataFrames\")\n\n    # Use reduce to perform the outer join on all DataFrames\n    joined_df = reduce(\n        lambda df1, df2: df1.join(df2, on=column_name, how=how), dataframes\n    )\n    return joined_df\n</code></pre>"},{"location":"utils/#pysparky.utils.split_dataframe_by_column","title":"<code>split_dataframe_by_column(sdf, split_column)</code>","text":"<p>Splits a DataFrame into multiple DataFrames based on distinct values in a specified column.</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>The input Spark DataFrame.</p> required <code>column_name</code> <code>str</code> <p>The column name to split the DataFrame by.</p> required <p>Returns:</p> Type Description <code>dict[str, DataFrame]</code> <p>dict[str, DataFrame]: A dictionary where keys are distinct column values and values are DataFrames.</p> Source code in <code>pysparky/utils.py</code> <pre><code>def split_dataframe_by_column(\n    sdf: DataFrame, split_column: str\n) -&gt; dict[str, DataFrame]:\n    \"\"\"\n    Splits a DataFrame into multiple DataFrames based on distinct values in a specified column.\n\n    Parameters:\n        sdf (DataFrame): The input Spark DataFrame.\n        column_name (str): The column name to split the DataFrame by.\n\n    Returns:\n        dict[str, DataFrame]: A dictionary where keys are distinct column values and values are DataFrames.\n    \"\"\"\n    # Get distinct values from the specified column\n    unique_values = [\n        row[split_column] for row in sdf.select(split_column).distinct().collect()\n    ]\n\n    # Create a dictionary to hold the filtered DataFrames\n    filtered_dfs = {\n        value: sdf.filter(F.col(split_column) == value) for value in unique_values\n    }\n\n    return filtered_dfs\n</code></pre>"},{"location":"utils/#pysparky.utils.split_dataframe_by_column_groups","title":"<code>split_dataframe_by_column_groups(df, column_name, *value_groups)</code>","text":"<p>Splits a DataFrame into multiple DataFrames based on groups of values in a specified column.</p> <p>Parameters: df (DataFrame): The input PySpark DataFrame. column_name (str): The column to split on. *value_groups (list): Variable number of value groups (each a list of values).</p> <p>Returns: tuple: A tuple of DataFrames, one for each group of values.</p> Source code in <code>pysparky/utils.py</code> <pre><code>def split_dataframe_by_column_groups(\n    df: DataFrame, column_name: str, *value_groups: list[str | list]\n) -&gt; tuple[DataFrame, ...]:\n    \"\"\"\n    Splits a DataFrame into multiple DataFrames based on groups of values in a specified column.\n\n    Parameters:\n    df (DataFrame): The input PySpark DataFrame.\n    column_name (str): The column to split on.\n    *value_groups (list): Variable number of value groups (each a list of values).\n\n    Returns:\n    tuple: A tuple of DataFrames, one for each group of values.\n    \"\"\"\n    return tuple(df.filter(df[column_name].isin(group)) for group in value_groups)\n</code></pre>"},{"location":"utils/#pysparky.utils.union_dataframes","title":"<code>union_dataframes(*dataframes)</code>","text":"<p>Unions a list of DataFrames.</p> <p>Parameters:</p> Name Type Description Default <code>*dataframes</code> <code>DataFrame</code> <p>A list of DataFrames to union.</p> <code>()</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>The resulting DataFrame after performing the unions.</p> Source code in <code>pysparky/utils.py</code> <pre><code>def union_dataframes(*dataframes: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Unions a list of DataFrames.\n\n    Args:\n        *dataframes (DataFrame): A list of DataFrames to union.\n\n    Returns:\n        DataFrame: The resulting DataFrame after performing the unions.\n    \"\"\"\n    # TODO: Check on the schema, if not align, raise error\n\n    if not dataframes:\n        raise ValueError(\"At least one DataFrame must be provided\")\n\n    return reduce(lambda df1, df2: df1.union(df2), dataframes)\n</code></pre>"},{"location":"functions/ai/","title":"LLM","text":""},{"location":"functions/ai/#pysparky.functions.ai.llm.build_text_generation_udf","title":"<code>build_text_generation_udf(model_bc, tokenizer_bc, system_prompt)</code>","text":"<p>Creates a Spark UDF for text generation using a Hugging Face model and tokenizer.</p> <p>This function sets up a user-defined function (UDF) that can be used in Spark DataFrames to perform text generation. It uses a broadcasted Hugging Face model and tokenizer to ensure efficient distribution across Spark workers.</p> <p>Parameters:</p> Name Type Description Default <code>model_bc</code> <code>Broadcast</code> <p>Broadcasted Hugging Face model.</p> required <code>tokenizer_bc</code> <code>Broadcast</code> <p>Broadcasted Hugging Face tokenizer.</p> required <code>system_prompt</code> <code>str</code> <p>Prompt to prepend to each input string before generation.</p> required <p>Returns:</p> Name Type Description <code>function</code> <code>UserDefinedFunction</code> <p>A Spark UDF that takes a string input and returns the generated text.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>model_bc</code> or <code>tokenizer_bc</code> is not a Broadcast instance.</p> Example <pre><code>&gt;&gt;&gt; from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n&gt;&gt;&gt; model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\")\n&gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-small\")\n&gt;&gt;&gt; t5_udf = build_text_generation_udf(\n...     sc.broadcast(model), sc.broadcast(tokenizer), \"sentiment of the text\"\n... )\n&gt;&gt;&gt; results_df = input_df.withColumn(\"output_column\", t5_udf(\"sentence\"))\n</code></pre> Source code in <code>pysparky/functions/ai/llm.py</code> <pre><code>def build_text_generation_udf(\n    model_bc: Broadcast, tokenizer_bc: Broadcast, system_prompt: str\n) -&gt; UserDefinedFunction:\n    \"\"\"Creates a Spark UDF for text generation using a Hugging Face model and tokenizer.\n\n    This function sets up a user-defined function (UDF) that can be used in Spark DataFrames\n    to perform text generation. It uses a broadcasted Hugging Face model and tokenizer to\n    ensure efficient distribution across Spark workers.\n\n    Args:\n        model_bc (Broadcast): Broadcasted Hugging Face model.\n        tokenizer_bc (Broadcast): Broadcasted Hugging Face tokenizer.\n        system_prompt (str): Prompt to prepend to each input string before generation.\n\n    Returns:\n        function: A Spark UDF that takes a string input and returns the generated text.\n\n    Raises:\n        TypeError: If `model_bc` or `tokenizer_bc` is not a Broadcast instance.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n        &gt;&gt;&gt; model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\")\n        &gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-small\")\n        &gt;&gt;&gt; t5_udf = build_text_generation_udf(\n        ...     sc.broadcast(model), sc.broadcast(tokenizer), \"sentiment of the text\"\n        ... )\n        &gt;&gt;&gt; results_df = input_df.withColumn(\"output_column\", t5_udf(\"sentence\"))\n        ```\n    \"\"\"\n    if not isinstance(model_bc, Broadcast):\n        raise TypeError(\n            \"model_bc must be a pyspark.Broadcast instance. \"\n            \"Broadcasting ensures the model is efficiently shared across Spark workers.\"\n        )\n    if not isinstance(tokenizer_bc, Broadcast):\n        raise TypeError(\n            \"tokenizer_bc must be a pyspark.Broadcast instance. \"\n            \"Broadcasting ensures the tokenizer is efficiently shared across Spark workers.\"\n        )\n\n    model: PreTrainedModel = model_bc.value\n    tokenizer: PreTrainedTokenizer = tokenizer_bc.value\n\n    @udf(StringType())\n    def generate_text_udf(user_input: str) -&gt; str:\n        \"\"\"Generates text from the model using the given input and system prompt.\"\"\"\n        full_input = f\"{system_prompt}: {user_input}\"\n        inputs = tokenizer(full_input, return_tensors=\"pt\")\n        outputs = model.generate(**inputs)\n        return tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n    return generate_text_udf\n</code></pre>"},{"location":"functions/cast/","title":"cast","text":""},{"location":"functions/cast/#pysparky.functions.cast.cast_string_to_boolean","title":"<code>cast_string_to_boolean(column_or_name)</code>","text":"<p>Casts a column of string values to boolean values.</p> <p>This function converts specific string representations of boolean values to their corresponding boolean types. The recognized string values for <code>False</code> are \"False\", \"false\", \"F\", \"f\", and \"0\". The recognized string values for <code>True</code> are \"True\", \"true\", \"T\", \"t\", and \"1\". Any other values will be converted to None.</p> <p>Parameters:</p> Name Type Description Default <code>column</code> <code>Column</code> <p>The input column containing string values to be cast.</p> required <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>A column with boolean values where recognized strings are</p> <code>Column</code> <p>converted to their corresponding boolean values, and unrecognized</p> <code>Column</code> <p>strings are converted to None.</p> Example <pre><code>&gt;&gt;&gt; df = spark.createDataFrame([(\"True\",), (\"false\",), (\"1\",), (\"0\",), (\"other\",)], [\"bool_str\"])\n&gt;&gt;&gt; df.select(cast_string_to_boolean(F.col(\"bool_str\")).alias(\"bool_val\")).show()\n+--------+\n|bool_val|\n+--------+\n|    true|\n|   false|\n|    true|\n|   false|\n|    null|\n+--------+\n</code></pre> Source code in <code>pysparky/functions/cast.py</code> <pre><code>def cast_string_to_boolean(column_or_name: ColumnOrName) -&gt; Column:\n    \"\"\"\n    Casts a column of string values to boolean values.\n\n    This function converts specific string representations of boolean values\n    to their corresponding boolean types. The recognized string values for\n    `False` are \"False\", \"false\", \"F\", \"f\", and \"0\". The recognized string\n    values for `True` are \"True\", \"true\", \"T\", \"t\", and \"1\". Any other values\n    will be converted to None.\n\n    Args:\n        column (Column): The input column containing string values to be cast.\n\n    Returns:\n        Column: A column with boolean values where recognized strings are\n        converted to their corresponding boolean values, and unrecognized\n        strings are converted to None.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; df = spark.createDataFrame([(\"True\",), (\"false\",), (\"1\",), (\"0\",), (\"other\",)], [\"bool_str\"])\n        &gt;&gt;&gt; df.select(cast_string_to_boolean(F.col(\"bool_str\")).alias(\"bool_val\")).show()\n        +--------+\n        |bool_val|\n        +--------+\n        |    true|\n        |   false|\n        |    true|\n        |   false|\n        |    null|\n        +--------+\n        ```\n    \"\"\"\n    (column,) = ensure_column(column_or_name)\n\n    false_string = [\"False\", \"false\", \"F\", \"f\", \"0\"]\n    true_string = [\"True\", \"true\", \"T\", \"t\", \"1\"]\n\n    return (\n        F.when(column.isin(false_string), False)\n        .when(column.isin(true_string), True)\n        .otherwise(None)\n    )\n</code></pre>"},{"location":"functions/cast/#pysparky.functions.cast.to_timestamps","title":"<code>to_timestamps(column_or_name, formats)</code>","text":"<p>Converts a column with date/time strings into a timestamp column by trying multiple formats.</p> <p>This function iterates over a list of date/time formats and attempts to parse the input column using each format. The first format that successfully parses the value is used. If no format succeeds, the result for that row is <code>NULL</code>.</p> <p>Parameters:</p> Name Type Description Default <code>column_or_name</code> <code>ColumnOrName</code> <p>The input Spark column containing date/time strings to be converted to timestamp format, or the column name.</p> required <code>formats</code> <code>list[str]</code> <p>A list of date/time format strings to try. Formats should follow the pattern conventions of <code>java.text.SimpleDateFormat</code>, such as \"yyyy-MM-dd\", \"MM/dd/yyyy\", etc.</p> required <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>A Spark Column of type timestamp. If none of the formats match for a row, the value will be <code>NULL</code>.</p> Example <pre><code>&gt;&gt;&gt; df = spark.createDataFrame([(\"2021-01-01\",), (\"01/02/2021\",), (\"invalid\",)], [\"date_str\"])\n&gt;&gt;&gt; formats = [\"yyyy-MM-dd\", \"MM/dd/yyyy\"]\n&gt;&gt;&gt; df.select(to_timestamps(F.col(\"date_str\"), formats).alias(\"timestamp\")).show()\n+-------------------+\n|          timestamp|\n+-------------------+\n|2021-01-01 00:00:00|\n|2021-01-02 00:00:00|\n|               null|\n+-------------------+\n</code></pre> Source code in <code>pysparky/functions/cast.py</code> <pre><code>def to_timestamps(column_or_name: ColumnOrName, formats: list[str]) -&gt; Column:\n    \"\"\"\n    Converts a column with date/time strings into a timestamp column by trying multiple formats.\n\n    This function iterates over a list of date/time formats and attempts to parse the input column\n    using each format. The first format that successfully parses the value is used. If no format succeeds,\n    the result for that row is `NULL`.\n\n    Args:\n        column_or_name (ColumnOrName): The input Spark column containing date/time strings to be converted to timestamp format,\n            or the column name.\n        formats (list[str]): A list of date/time format strings to try. Formats should follow the pattern\n            conventions of `java.text.SimpleDateFormat`, such as \"yyyy-MM-dd\", \"MM/dd/yyyy\", etc.\n\n    Returns:\n        Column: A Spark Column of type timestamp. If none of the formats match for a row, the value will be `NULL`.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; df = spark.createDataFrame([(\"2021-01-01\",), (\"01/02/2021\",), (\"invalid\",)], [\"date_str\"])\n        &gt;&gt;&gt; formats = [\"yyyy-MM-dd\", \"MM/dd/yyyy\"]\n        &gt;&gt;&gt; df.select(to_timestamps(F.col(\"date_str\"), formats).alias(\"timestamp\")).show()\n        +-------------------+\n        |          timestamp|\n        +-------------------+\n        |2021-01-01 00:00:00|\n        |2021-01-02 00:00:00|\n        |               null|\n        +-------------------+\n        ```\n    \"\"\"\n    (column,) = ensure_column(column_or_name)\n\n    def reducer(acc, format):\n        format_col = F.lit(format)\n        return acc.when(\n            # this will supress the error\n            F.try_to_timestamp(column, format_col).isNotNull(),\n            F.try_to_timestamp(column, format_col),\n        )\n\n    return reduce(reducer, formats, F).otherwise(\n        # This follows spark.sql.ansi.enabled behavior\n        F.to_timestamp(column)\n    )\n</code></pre>"},{"location":"functions/conditions/","title":"conditions","text":""},{"location":"functions/conditions/#pysparky.functions.conditions.condition_and","title":"<code>condition_and(*conditions)</code>","text":"<p>Combines multiple conditions using logical AND.</p> <p>Parameters:</p> Name Type Description Default <code>*conditions</code> <code>ColumnOrName</code> <p>Multiple PySpark Column objects or SQL expression strings representing conditions.</p> <code>()</code> <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>A single PySpark Column object representing the combined condition.</p> Example <pre><code>&gt;&gt;&gt; condition_and(F.col('col1') &gt; 1, F.col('col2') &lt; 5)\nColumn&lt;'((col1 &gt; 1) AND (col2 &lt; 5))'&gt;\n\n&gt;&gt;&gt; condition_and(F.col('col1') &gt; 1, \"col2 &lt; 5\")\nColumn&lt;'((col1 &gt; 1) AND (col2 &lt; 5))'&gt;\n</code></pre> Source code in <code>pysparky/functions/conditions.py</code> <pre><code>def condition_and(*conditions: ColumnOrName) -&gt; Column:\n    \"\"\"\n    Combines multiple conditions using logical AND.\n\n    Args:\n        *conditions (ColumnOrName): Multiple PySpark Column objects or SQL expression strings representing conditions.\n\n    Returns:\n        Column: A single PySpark Column object representing the combined condition.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; condition_and(F.col('col1') &gt; 1, F.col('col2') &lt; 5)\n        Column&lt;'((col1 &gt; 1) AND (col2 &lt; 5))'&gt;\n\n        &gt;&gt;&gt; condition_and(F.col('col1') &gt; 1, \"col2 &lt; 5\")\n        Column&lt;'((col1 &gt; 1) AND (col2 &lt; 5))'&gt;\n        ```\n    \"\"\"\n    parsed_conditions = [\n        F.expr(cond) if isinstance(cond, str) else cond for cond in conditions\n    ]\n    return reduce(and_, parsed_conditions, F.lit(True))\n</code></pre>"},{"location":"functions/conditions/#pysparky.functions.conditions.condition_or","title":"<code>condition_or(*conditions)</code>","text":"<p>Combines multiple conditions using logical OR.</p> <p>Parameters:</p> Name Type Description Default <code>*conditions</code> <code>ColumnOrName</code> <p>Multiple PySpark Column objects or SQL expression strings representing conditions.</p> <code>()</code> <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>A single PySpark Column object representing the combined condition.</p> Example <pre><code>&gt;&gt;&gt; condition_or(F.col('col1') &gt; 1, F.col('col2') &lt; 5)\nColumn&lt;'((col1 &gt; 1) OR (col2 &lt; 5))'&gt;\n\n&gt;&gt;&gt; condition_or(F.col('col1') &gt; 1, \"col2 &lt; 5\")\nColumn&lt;'((col1 &gt; 1) OR (col2 &lt; 5))'&gt;\n</code></pre> Source code in <code>pysparky/functions/conditions.py</code> <pre><code>def condition_or(*conditions: ColumnOrName) -&gt; Column:\n    \"\"\"\n    Combines multiple conditions using logical OR.\n\n    Args:\n        *conditions (ColumnOrName): Multiple PySpark Column objects or SQL expression strings representing conditions.\n\n    Returns:\n        Column: A single PySpark Column object representing the combined condition.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; condition_or(F.col('col1') &gt; 1, F.col('col2') &lt; 5)\n        Column&lt;'((col1 &gt; 1) OR (col2 &lt; 5))'&gt;\n\n        &gt;&gt;&gt; condition_or(F.col('col1') &gt; 1, \"col2 &lt; 5\")\n        Column&lt;'((col1 &gt; 1) OR (col2 &lt; 5))'&gt;\n        ```\n    \"\"\"\n    parsed_conditions = [\n        F.expr(cond) if isinstance(cond, str) else cond for cond in conditions\n    ]\n    return reduce(or_, parsed_conditions, F.lit(False))\n</code></pre>"},{"location":"functions/conditions/#pysparky.functions.conditions.is_all_numbers_only","title":"<code>is_all_numbers_only(column_or_name)</code>","text":"<p>Checks if the given column or string contains only numeric characters.</p> <p>Parameters:</p> Name Type Description Default <code>column_or_name</code> <code>ColumnOrName</code> <p>The column or string to be checked.</p> required <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>A column of boolean values indicating whether each entry contains only numeric characters.</p> Example <pre><code>&gt;&gt;&gt; df = spark.createDataFrame([(\"123\",), (\"4567\",), (\"89a\",), (\"\",), (\"0\",)], [\"value\"])\n&gt;&gt;&gt; df.select(is_all_numbers_only(df[\"value\"]).alias(\"is_all_numbers\")).show()\n+--------------+\n|is_all_numbers|\n+--------------+\n|          true|\n|          true|\n|         false|\n|         false|\n|          true|\n+--------------+\n</code></pre> Source code in <code>pysparky/functions/conditions.py</code> <pre><code>def is_all_numbers_only(column_or_name) -&gt; Column:\n    \"\"\"\n    Checks if the given column or string contains only numeric characters.\n\n    Args:\n        column_or_name (ColumnOrName): The column or string to be checked.\n\n    Returns:\n        Column: A column of boolean values indicating whether each entry contains only numeric characters.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; df = spark.createDataFrame([(\"123\",), (\"4567\",), (\"89a\",), (\"\",), (\"0\",)], [\"value\"])\n        &gt;&gt;&gt; df.select(is_all_numbers_only(df[\"value\"]).alias(\"is_all_numbers\")).show()\n        +--------------+\n        |is_all_numbers|\n        +--------------+\n        |          true|\n        |          true|\n        |         false|\n        |         false|\n        |          true|\n        +--------------+\n        ```\n    \"\"\"\n    return is_n_numbers_only(column_or_name, n=\"+\")\n</code></pre>"},{"location":"functions/conditions/#pysparky.functions.conditions.is_n_character_only","title":"<code>is_n_character_only(column_or_name, n)</code>","text":"<p>Checks if the given column or string contains exactly <code>n</code> alphabetic characters.</p> <p>Parameters:</p> Name Type Description Default <code>column_or_name</code> <code>Column</code> <p>The column or string to be checked.</p> required <code>n</code> <code>int</code> <p>The exact number of alphabetic characters to match.</p> required <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>A column of boolean values indicating whether each entry matches the regular expression.</p> Example <pre><code>&gt;&gt;&gt; df = spark.createDataFrame([(\"a\",), (\"ab\",), (\"abc\",), (\"12\",)], [\"value\"])\n&gt;&gt;&gt; df.select(is_n_character_only(df[\"value\"], 2).alias(\"is_two_char\")).show()\n+-----------+\n|is_two_char|\n+-----------+\n|      false|\n|       true|\n|      false|\n|      false|\n+-----------+\n</code></pre> Source code in <code>pysparky/functions/conditions.py</code> <pre><code>def is_n_character_only(column_or_name: ColumnOrName, n: int) -&gt; Column:\n    \"\"\"\n    Checks if the given column or string contains exactly `n` alphabetic characters.\n\n    Args:\n        column_or_name (Column): The column or string to be checked.\n        n (int): The exact number of alphabetic characters to match.\n\n    Returns:\n        Column: A column of boolean values indicating whether each entry matches the regular expression.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; df = spark.createDataFrame([(\"a\",), (\"ab\",), (\"abc\",), (\"12\",)], [\"value\"])\n        &gt;&gt;&gt; df.select(is_n_character_only(df[\"value\"], 2).alias(\"is_two_char\")).show()\n        +-----------+\n        |is_two_char|\n        +-----------+\n        |      false|\n        |       true|\n        |      false|\n        |      false|\n        +-----------+\n        ```\n    \"\"\"\n    # double curly braces {{ }} to escape the braces in the f-string\n    regexp = rf\"^[a-zA-Z]{{{n}}}$\"\n    return F.regexp_like(column_or_name, regexp=F.lit(regexp))\n</code></pre>"},{"location":"functions/conditions/#pysparky.functions.conditions.is_n_numbers_only","title":"<code>is_n_numbers_only(column_or_name, n)</code>","text":"<p>Checks if the given column or string contains exactly <code>n</code> numeric characters.</p> <p>Parameters:</p> Name Type Description Default <code>column_or_name</code> <code>ColumnOrName</code> <p>The column or string to be checked.</p> required <code>n</code> <code>int | str</code> <p>The exact number of numeric characters to match. or \"+\" for any length number.</p> required <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>A column of boolean values indicating whether each entry matches the regular expression.</p> Example <pre><code>&gt;&gt;&gt; df = spark.createDataFrame([(\"123\",), (\"4567\",), (\"89a\",), (\"\",), (\"0\",)], [\"value\"])\n&gt;&gt;&gt; df.select(is_n_numbers_only(df[\"value\"], 3).alias(\"is_n_numbers\")).show()\n+------------+\n|is_n_numbers|\n+------------+\n|        true|\n|       false|\n|       false|\n|       false|\n|       false|\n+------------+\n</code></pre> Source code in <code>pysparky/functions/conditions.py</code> <pre><code>def is_n_numbers_only(column_or_name: ColumnOrName, n: int | str) -&gt; Column:\n    \"\"\"\n    Checks if the given column or string contains exactly `n` numeric characters.\n\n    Args:\n        column_or_name (ColumnOrName): The column or string to be checked.\n        n (int | str): The exact number of numeric characters to match. or \"+\" for any length number.\n\n    Returns:\n        Column: A column of boolean values indicating whether each entry matches the regular expression.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; df = spark.createDataFrame([(\"123\",), (\"4567\",), (\"89a\",), (\"\",), (\"0\",)], [\"value\"])\n        &gt;&gt;&gt; df.select(is_n_numbers_only(df[\"value\"], 3).alias(\"is_n_numbers\")).show()\n        +------------+\n        |is_n_numbers|\n        +------------+\n        |        true|\n        |       false|\n        |       false|\n        |       false|\n        |       false|\n        +------------+\n        ```\n    \"\"\"\n    if isinstance(n, int):\n        # double curly braces {{ }} to escape the braces in the f-string\n        regexp = rf\"^\\d{{{n}}}$\"\n    elif n == \"+\":\n        # Any length number\n        regexp = r\"^\\d+$\"\n    else:\n        raise ValueError(\n            \"The parameter 'n' must be either an integer or the string '+'.\"\n        )\n    return F.regexp_like(column_or_name, F.lit(regexp))\n</code></pre>"},{"location":"functions/conditions/#pysparky.functions.conditions.is_printable_only","title":"<code>is_printable_only(column_or_name)</code>","text":"<p>Checks if the given column or string contains only printable characters.</p> <p>Parameters:</p> Name Type Description Default <code>column_or_name</code> <code>ColumnOrName</code> <p>The column or string to be checked.</p> required <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>A column of boolean values indicating whether each entry contains only printable characters.</p> Example <pre><code>&gt;&gt;&gt; df = spark.createDataFrame([(\"Hello!\",), (\"World\",), (\"123\",), (\"\",), (\"Non-printable\\x01\",)], [\"value\"])\n&gt;&gt;&gt; df.select(is_printable_only(df[\"value\"]).alias(\"is_printable\")).show()\n+------------+\n|is_printable|\n+------------+\n|        true|\n|        true|\n|        true|\n|       false|\n|       false|\n+------------+\n</code></pre> Source code in <code>pysparky/functions/conditions.py</code> <pre><code>def is_printable_only(column_or_name: ColumnOrName) -&gt; Column:\n    \"\"\"\n    Checks if the given column or string contains only printable characters.\n\n    Args:\n        column_or_name (ColumnOrName): The column or string to be checked.\n\n    Returns:\n        Column: A column of boolean values indicating whether each entry contains only printable characters.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; df = spark.createDataFrame([(\"Hello!\",), (\"World\",), (\"123\",), (\"\",), (\"Non-printable\\\\x01\",)], [\"value\"])\n        &gt;&gt;&gt; df.select(is_printable_only(df[\"value\"]).alias(\"is_printable\")).show()\n        +------------+\n        |is_printable|\n        +------------+\n        |        true|\n        |        true|\n        |        true|\n        |       false|\n        |       false|\n        +------------+\n        ```\n    \"\"\"\n    # Regular expression for printable ASCII characters (0x20 to 0x7E)\n    regexp = r\"^[\\x20-\\x7E]+$\"\n    return F.regexp_like(column_or_name, F.lit(regexp))\n</code></pre>"},{"location":"functions/conditions/#pysparky.functions.conditions.is_two_character_only","title":"<code>is_two_character_only(column_or_name)</code>","text":"<p>Checks if the given column or string contains exactly two alphabetic characters (either lowercase or uppercase).</p> <p>Parameters:</p> Name Type Description Default <code>column_or_name</code> <code>ColumnOrName</code> <p>The column or string to be checked.</p> required <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>A boolean column indicating whether the input matches the pattern of exactly two alphabetic characters.</p> Example <pre><code>&gt;&gt;&gt; df = spark.createDataFrame([(\"aa\",), (\"ZZ\",), (\"a1\",), (\"abc\",)], [\"value\"])\n&gt;&gt;&gt; df.select(is_two_character_only(df[\"value\"]).alias(\"is_two_char\")).show()\n+-----------+\n|is_two_char|\n+-----------+\n|       true|\n|       true|\n|      false|\n|      false|\n+-----------+\n</code></pre> Source code in <code>pysparky/functions/conditions.py</code> <pre><code>def is_two_character_only(column_or_name: ColumnOrName) -&gt; Column:\n    \"\"\"\n    Checks if the given column or string contains exactly two alphabetic characters (either lowercase or uppercase).\n\n    Args:\n        column_or_name (ColumnOrName): The column or string to be checked.\n\n    Returns:\n        Column: A boolean column indicating whether the input matches the pattern of exactly two alphabetic characters.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; df = spark.createDataFrame([(\"aa\",), (\"ZZ\",), (\"a1\",), (\"abc\",)], [\"value\"])\n        &gt;&gt;&gt; df.select(is_two_character_only(df[\"value\"]).alias(\"is_two_char\")).show()\n        +-----------+\n        |is_two_char|\n        +-----------+\n        |       true|\n        |       true|\n        |      false|\n        |      false|\n        +-----------+\n        ```\n    \"\"\"\n    return is_n_character_only(column_or_name, n=2)\n</code></pre>"},{"location":"functions/conditions/#pysparky.functions.conditions.startswiths","title":"<code>startswiths(column_or_name, list_of_strings)</code>","text":"<p>Creates a PySpark Column expression to check if the given column starts with any string in the list.</p> <p>Parameters:</p> Name Type Description Default <code>column_or_name</code> <code>ColumnOrName</code> <p>The column to check.</p> required <code>list_of_strings</code> <code>List[str]</code> <p>A list of strings to check if the column starts with.</p> required <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>A PySpark Column expression that evaluates to True if the column starts with any string in the list, otherwise False.</p> Example <pre><code>&gt;&gt;&gt; df = spark.createDataFrame([(\"apple\",), (\"banana\",), (\"cherry\",)], [\"fruit\"])\n&gt;&gt;&gt; df.select(\"fruit\", startswiths(F.col(\"fruit\"), [\"ap\", \"ch\"]).alias(\"starts_with\")).show()\n+------+-----------+\n| fruit|starts_with|\n+------+-----------+\n| apple|       true|\n|banana|      false|\n|cherry|       true|\n+------+-----------+\n</code></pre> Source code in <code>pysparky/functions/conditions.py</code> <pre><code>@decorator.extension_enabler(Column)\ndef startswiths(\n    column_or_name: ColumnOrName, list_of_strings: list[str]\n) -&gt; pyspark.sql.Column:\n    \"\"\"\n    Creates a PySpark Column expression to check if the given column starts with any string in the list.\n\n    Args:\n        column_or_name (ColumnOrName): The column to check.\n        list_of_strings (List[str]): A list of strings to check if the column starts with.\n\n    Returns:\n        Column: A PySpark Column expression that evaluates to True if the column starts with any string in the list, otherwise False.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; df = spark.createDataFrame([(\"apple\",), (\"banana\",), (\"cherry\",)], [\"fruit\"])\n        &gt;&gt;&gt; df.select(\"fruit\", startswiths(F.col(\"fruit\"), [\"ap\", \"ch\"]).alias(\"starts_with\")).show()\n        +------+-----------+\n        | fruit|starts_with|\n        +------+-----------+\n        | apple|       true|\n        |banana|      false|\n        |cherry|       true|\n        +------+-----------+\n        ```\n    \"\"\"\n    (column,) = ensure_column(column_or_name)\n\n    return reduce(\n        or_,\n        map(column.startswith, list_of_strings),\n        F.lit(False),\n    ).alias(f\"startswiths_len{len(list_of_strings)}\")\n</code></pre>"},{"location":"functions/general/","title":"general","text":""},{"location":"functions/general/#pysparky.functions.general.chain","title":"<code>chain(self, func, *args, **kwargs)</code>","text":"<p>Applies a given function to the current Column and returns the result.</p> <p>This method allows for chaining operations on a Column object by applying a custom function with additional arguments. It's particularly useful for creating complex transformations or applying user-defined functions to a Column.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <code>Column</code> <p>The current Column object.</p> required <code>func</code> <code>callable</code> <p>The function to apply to the Column.</p> required <code>*args</code> <p>Variable length argument list to pass to the function.</p> <code>()</code> <code>**kwargs</code> <p>Arbitrary keyword arguments to pass to the function.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>A new Column object resulting from applying the function.</p> Example <pre><code>&gt;&gt;&gt; df = spark.createDataFrame([(\"hello\",)], [\"text\"])\n&gt;&gt;&gt; def custom_upper(col):\n...     return F.upper(col)\n&gt;&gt;&gt; result = df.withColumn(\"upper_text\", df.text.chain(custom_upper))\n&gt;&gt;&gt; result.show()\n+-----+----------+\n| text|upper_text|\n+-----+----------+\n|hello|     HELLO|\n+-----+----------+\n\n&gt;&gt;&gt; def add_prefix(col, prefix):\n...     return F.concat(F.lit(prefix), col)\n&gt;&gt;&gt; result = df.withColumn(\"prefixed_text\", df.text.chain(add_prefix, prefix=\"Pre: \"))\n&gt;&gt;&gt; result.show()\n+-----+-------------+\n| text|prefixed_text|\n+-----+-------------+\n|hello|   Pre: hello|\n+-----+-------------+\n</code></pre> Note <p>The function passed to <code>chain</code> should expect a Column as its first argument, followed by any additional arguments specified in the <code>chain</code> call.</p> Source code in <code>pysparky/functions/general.py</code> <pre><code>@decorator.extension_enabler(Column)\ndef chain(self, func, *args, **kwargs) -&gt; Column:\n    \"\"\"\n    Applies a given function to the current Column and returns the result.\n\n    This method allows for chaining operations on a Column object by applying\n    a custom function with additional arguments. It's particularly useful for\n    creating complex transformations or applying user-defined functions to a Column.\n\n    Args:\n        self (Column): The current Column object.\n        func (callable): The function to apply to the Column.\n        *args: Variable length argument list to pass to the function.\n        **kwargs: Arbitrary keyword arguments to pass to the function.\n\n    Returns:\n        Column: A new Column object resulting from applying the function.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; df = spark.createDataFrame([(\"hello\",)], [\"text\"])\n        &gt;&gt;&gt; def custom_upper(col):\n        ...     return F.upper(col)\n        &gt;&gt;&gt; result = df.withColumn(\"upper_text\", df.text.chain(custom_upper))\n        &gt;&gt;&gt; result.show()\n        +-----+----------+\n        | text|upper_text|\n        +-----+----------+\n        |hello|     HELLO|\n        +-----+----------+\n\n        &gt;&gt;&gt; def add_prefix(col, prefix):\n        ...     return F.concat(F.lit(prefix), col)\n        &gt;&gt;&gt; result = df.withColumn(\"prefixed_text\", df.text.chain(add_prefix, prefix=\"Pre: \"))\n        &gt;&gt;&gt; result.show()\n        +-----+-------------+\n        | text|prefixed_text|\n        +-----+-------------+\n        |hello|   Pre: hello|\n        +-----+-------------+\n        ```\n\n    Note:\n        The function passed to `chain` should expect a Column as its first argument,\n        followed by any additional arguments specified in the `chain` call.\n    \"\"\"\n    return func(self, *args, **kwargs)\n</code></pre>"},{"location":"functions/general/#pysparky.functions.general.get_value_from_map","title":"<code>get_value_from_map(column_or_name, dict_)</code>","text":"<p>Retrieves a value from a map (dictionary) using a key derived from a specified column in a DataFrame.</p> <p>This function creates a map from the provided dictionary and then looks up the value in the map corresponding to the key that matches the value in the specified column.</p> <p>Parameters:</p> Name Type Description Default <code>column_or_name</code> <code>str</code> <p>The name of the column in the DataFrame whose value will be used as the key to look up in the map.</p> required <code>dict_</code> <code>dict</code> <p>A dictionary where keys and values are the elements to be used in the map.</p> required <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>A PySpark Column object representing the value retrieved from the map.</p> Example <pre><code>&gt;&gt;&gt; map = {1: 'a', 2: 'b'}\n&gt;&gt;&gt; column_name = 'key_column'\n&gt;&gt;&gt; df = spark.createDataFrame([(1,), (2,)], ['key_column'])\n&gt;&gt;&gt; df.withColumn('value', get_value_from_map(map, column_name)).show()\n+----------+-----+\n|key_column|value|\n+----------+-----+\n|         1|    a|\n|         2|    b|\n+----------+-----+\n</code></pre> Source code in <code>pysparky/functions/general.py</code> <pre><code>@decorator.extension_enabler(Column)\ndef get_value_from_map(column_or_name: ColumnOrName, dict_: dict) -&gt; Column:\n    \"\"\"\n    Retrieves a value from a map (dictionary) using a key derived from a specified column in a DataFrame.\n\n    This function creates a map from the provided dictionary and then looks up the value in the map\n    corresponding to the key that matches the value in the specified column.\n\n    Args:\n        column_or_name (str): The name of the column in the DataFrame whose value will be used as the key to look up in the map.\n        dict_ (dict): A dictionary where keys and values are the elements to be used in the map.\n\n    Returns:\n        Column: A PySpark Column object representing the value retrieved from the map.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; map = {1: 'a', 2: 'b'}\n        &gt;&gt;&gt; column_name = 'key_column'\n        &gt;&gt;&gt; df = spark.createDataFrame([(1,), (2,)], ['key_column'])\n        &gt;&gt;&gt; df.withColumn('value', get_value_from_map(map, column_name)).show()\n        +----------+-----+\n        |key_column|value|\n        +----------+-----+\n        |         1|    a|\n        |         2|    b|\n        +----------+-----+\n        ```\n    \"\"\"\n    (column,) = ensure_column(column_or_name)\n\n    return utils.create_map_from_dict(dict_)[column]\n</code></pre>"},{"location":"functions/general/#pysparky.functions.general.lower_","title":"<code>lower_(col)</code>","text":"<p>This serve as an easy Examples on how this package work</p> <p>Parameters:</p> Name Type Description Default <code>col</code> <code>Column</code> <p>The column to be lowercased.</p> required <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>A lowercased column.</p> Example <pre><code>&gt;&gt;&gt; df = spark.createDataFrame([(\"Hello\",)], [\"text\"])\n&gt;&gt;&gt; df.select(lower_(F.col(\"text\"))).show()\n+-----------+\n|lower(text)|\n+-----------+\n|      hello|\n+-----------+\n</code></pre> Source code in <code>pysparky/functions/general.py</code> <pre><code>@decorator.extension_enabler(Column)\ndef lower_(col: Column) -&gt; Column:\n    \"\"\"\n    This serve as an easy Examples on how this package work\n\n    Args:\n        col (Column): The column to be lowercased.\n\n    Returns:\n        Column: A lowercased column.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; df = spark.createDataFrame([(\"Hello\",)], [\"text\"])\n        &gt;&gt;&gt; df.select(lower_(F.col(\"text\"))).show()\n        +-----------+\n        |lower(text)|\n        +-----------+\n        |      hello|\n        +-----------+\n        ```\n    \"\"\"\n    return F.lower(col)\n</code></pre>"},{"location":"functions/general/#pysparky.functions.general.replace_strings_to_none","title":"<code>replace_strings_to_none(column_or_name, list_of_null_string, customize_output=None)</code>","text":"<p>Replaces empty string values in a column with None.</p> <p>Parameters:</p> Name Type Description Default <code>column_or_name</code> <code>ColumnOrName</code> <p>The column to check for empty string values.</p> required <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>A Spark DataFrame column with the values replaced.</p> Example <pre><code>&gt;&gt;&gt; df = spark.createDataFrame([(\"\",), (\"foo\",), (None,)], [\"col\"])\n&gt;&gt;&gt; df.select(replace_strings_to_none(F.col(\"col\"), [\"\"]).alias(\"cleaned\")).show()\n+-------+\n|cleaned|\n+-------+\n|   null|\n|    foo|\n|   null|\n+-------+\n</code></pre> Source code in <code>pysparky/functions/general.py</code> <pre><code>@decorator.extension_enabler(Column)\ndef replace_strings_to_none(\n    column_or_name: ColumnOrName,\n    list_of_null_string: list[str],\n    customize_output: Any = None,\n) -&gt; pyspark.sql.Column:\n    \"\"\"\n    Replaces empty string values in a column with None.\n\n    Args:\n        column_or_name (ColumnOrName): The column to check for empty string values.\n\n    Returns:\n        Column: A Spark DataFrame column with the values replaced.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; df = spark.createDataFrame([(\"\",), (\"foo\",), (None,)], [\"col\"])\n        &gt;&gt;&gt; df.select(replace_strings_to_none(F.col(\"col\"), [\"\"]).alias(\"cleaned\")).show()\n        +-------+\n        |cleaned|\n        +-------+\n        |   null|\n        |    foo|\n        |   null|\n        +-------+\n        ```\n    \"\"\"\n\n    (column,) = ensure_column(column_or_name)\n\n    return F.when(column.isin(list_of_null_string), customize_output).otherwise(column)\n</code></pre>"},{"location":"functions/general/#pysparky.functions.general.single_space_and_trim","title":"<code>single_space_and_trim(column_or_name)</code>","text":"<p>Replaces multiple white spaces with a single space and trims the column.</p> <p>Parameters:</p> Name Type Description Default <code>column_or_name</code> <code>Column</code> <p>The column to be adjusted.</p> required <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>A trimmed column with single spaces.</p> Example <pre><code>&gt;&gt;&gt; df = spark.createDataFrame([(\"  foo   bar  \",)], [\"text\"])\n&gt;&gt;&gt; df.select(single_space_and_trim(F.col(\"text\")).alias(\"cleaned\")).show()\n+-------+\n|cleaned|\n+-------+\n|foo bar|\n+-------+\n</code></pre> Source code in <code>pysparky/functions/general.py</code> <pre><code>@decorator.extension_enabler(Column)\ndef single_space_and_trim(column_or_name: ColumnOrName) -&gt; Column:\n    \"\"\"\n    Replaces multiple white spaces with a single space and trims the column.\n\n    Args:\n        column_or_name (Column): The column to be adjusted.\n\n    Returns:\n        Column: A trimmed column with single spaces.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; df = spark.createDataFrame([(\"  foo   bar  \",)], [\"text\"])\n        &gt;&gt;&gt; df.select(single_space_and_trim(F.col(\"text\")).alias(\"cleaned\")).show()\n        +-------+\n        |cleaned|\n        +-------+\n        |foo bar|\n        +-------+\n        ```\n    \"\"\"\n\n    return F.trim(F.regexp_replace(column_or_name, r\"\\s+\", \" \"))\n</code></pre>"},{"location":"functions/general/#pysparky.functions.general.when_mapping","title":"<code>when_mapping(column_or_name, dict_)</code>","text":"<p>Applies a series of conditional mappings to a PySpark Column based on a dictionary of conditions and values.</p> <p>Parameters:</p> Name Type Description Default <code>column</code> <code>Column</code> <p>The PySpark Column to which the conditional mappings will be applied.</p> required <code>dict_</code> <code>Dict</code> <p>A dictionary where keys are the conditions and values are the corresponding results.</p> required <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>A new PySpark Column with the conditional mappings applied.</p> Example <pre><code>&gt;&gt;&gt; df = spark.createDataFrame([(\"A\",), (\"B\",), (\"C\",)], [\"category\"])\n&gt;&gt;&gt; mapping = {\"A\": 1, \"B\": 2}\n&gt;&gt;&gt; df.select(\"category\", when_mapping(F.col(\"category\"), mapping).alias(\"mapped\")).show()\n+--------+------+\n|category|mapped|\n+--------+------+\n|       A|     1|\n|       B|     2|\n|       C|  null|\n+--------+------+\n</code></pre> Source code in <code>pysparky/functions/general.py</code> <pre><code>@decorator.extension_enabler(Column)\ndef when_mapping(column_or_name: ColumnOrName, dict_: dict) -&gt; Column:\n    \"\"\"\n    Applies a series of conditional mappings to a PySpark Column based on a dictionary of conditions and values.\n\n    Args:\n        column (Column): The PySpark Column to which the conditional mappings will be applied.\n        dict_ (Dict): A dictionary where keys are the conditions and values are the corresponding results.\n\n    Returns:\n        Column: A new PySpark Column with the conditional mappings applied.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; df = spark.createDataFrame([(\"A\",), (\"B\",), (\"C\",)], [\"category\"])\n        &gt;&gt;&gt; mapping = {\"A\": 1, \"B\": 2}\n        &gt;&gt;&gt; df.select(\"category\", when_mapping(F.col(\"category\"), mapping).alias(\"mapped\")).show()\n        +--------+------+\n        |category|mapped|\n        +--------+------+\n        |       A|     1|\n        |       B|     2|\n        |       C|  null|\n        +--------+------+\n        ```\n    \"\"\"\n    (column,) = ensure_column(column_or_name)\n\n    def reducer(result_column: Column, condition_value: tuple[Any, Any]) -&gt; Column:\n        condition, value = condition_value\n        return result_column.when(column == condition, value)\n\n    result_column: Column = functools.reduce(reducer, dict_.items(), F)  # type: ignore\n    return result_column\n</code></pre>"},{"location":"functions/math/","title":"math_","text":""},{"location":"functions/math/#pysparky.functions.math_.cumsum","title":"<code>cumsum(column_or_name, partition_by=None, order_by_column=None, is_normalized=False, is_descending=False, alias='cumsum')</code>","text":"<p>Calculate the cumulative sum of a column, optionally partitioned by other columns.</p> <p>Parameters:</p> Name Type Description Default <code>column_or_name</code> <code>Column</code> <p>The column for which to calculate the cumulative sum.</p> required <code>partition_by</code> <code>list[Column]</code> <p>A list of columns to partition by. Defaults to an empty list.</p> <code>None</code> <code>order_by_column</code> <code>Column | None</code> <p>The Column for order by, null for using the same column.</p> <code>None</code> <code>is_normalized</code> <code>bool</code> <p>Whether to normalize the cumulative sum. Defaults to False.</p> <code>False</code> <code>is_descending</code> <code>bool</code> <p>Whether to order the cumulative sum in descending order. Defaults to False.</p> <code>False</code> <code>alias</code> <code>str</code> <p>Alias for the resulting column. Defaults to \"cumsum\".</p> <code>'cumsum'</code> <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>A column representing the cumulative sum.</p> Example <pre><code>&gt;&gt;&gt; df = spark.createDataFrame([(1, \"A\", 10), (2, \"A\", 20), (3, \"B\", 30)], [\"id\", \"category\", \"value\"])\n&gt;&gt;&gt; result_df = df.select(\"id\", \"category\", \"value\", cumsum(F.col(\"value\"), partition_by=[F.col(\"category\")], is_descending=True))\n&gt;&gt;&gt; result_df.show()\n+---+--------+-----+------+\n| id|category|value|cumsum|\n+---+--------+-----+------+\n|  1|       A|   10|    30|\n|  2|       A|   20|    20|\n|  3|       B|   30|    30|\n+---+--------+-----+------+\n</code></pre> Source code in <code>pysparky/functions/math_.py</code> <pre><code>def cumsum(  # pylint: disable=too-many-positional-arguments\n    column_or_name: ColumnOrName,\n    partition_by: list[Column] | None = None,\n    order_by_column: Column | None = None,\n    is_normalized: bool = False,\n    is_descending: bool = False,\n    alias: str = \"cumsum\",\n) -&gt; Column:\n    \"\"\"\n    Calculate the cumulative sum of a column, optionally partitioned by other columns.\n\n    Args:\n        column_or_name (Column): The column for which to calculate the cumulative sum.\n        partition_by (list[Column], optional): A list of columns to partition by. Defaults to an empty list.\n        order_by_column: The Column for order by, null for using the same column.\n        is_normalized (bool, optional): Whether to normalize the cumulative sum. Defaults to False.\n        is_descending (bool, optional): Whether to order the cumulative sum in descending order. Defaults to False.\n        alias (str, optional): Alias for the resulting column. Defaults to \"cumsum\".\n\n    Returns:\n        Column: A column representing the cumulative sum.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; df = spark.createDataFrame([(1, \"A\", 10), (2, \"A\", 20), (3, \"B\", 30)], [\"id\", \"category\", \"value\"])\n        &gt;&gt;&gt; result_df = df.select(\"id\", \"category\", \"value\", cumsum(F.col(\"value\"), partition_by=[F.col(\"category\")], is_descending=True))\n        &gt;&gt;&gt; result_df.show()\n        +---+--------+-----+------+\n        | id|category|value|cumsum|\n        +---+--------+-----+------+\n        |  1|       A|   10|    30|\n        |  2|       A|   20|    20|\n        |  3|       B|   30|    30|\n        +---+--------+-----+------+\n        ```\n    \"\"\"\n    (column,) = ensure_column(column_or_name)\n\n    if partition_by is None:\n        partition_by = []\n    if order_by_column is None:\n        order_by_column = column\n\n    if is_normalized:\n        total_sum = F.sum(column).over(Window.partitionBy(partition_by))\n    else:\n        total_sum = F.lit(1)\n\n    if is_descending:\n        order_by_column_ordered = order_by_column.desc()\n    else:\n        order_by_column_ordered = order_by_column.asc()\n\n    cumsum_ = F.sum(column).over(\n        Window.partitionBy(partition_by).orderBy(order_by_column_ordered)\n    )\n\n    return (cumsum_ / total_sum).alias(alias)\n</code></pre>"},{"location":"functions/math/#pysparky.functions.math_.haversine_distance","title":"<code>haversine_distance(lat1, long1, lat2, long2)</code>","text":"<p>Calculates the Haversine distance between two sets of latitude and longitude coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>lat1</code> <code>ColumnOrName</code> <p>The column containing the latitude of the first coordinate.</p> required <code>long1</code> <code>ColumnOrName</code> <p>The column containing the longitude of the first coordinate.</p> required <code>lat2</code> <code>ColumnOrName</code> <p>The column containing the latitude of the second coordinate.</p> required <code>long2</code> <code>ColumnOrName</code> <p>The column containing the longitude of the second coordinate.</p> required <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>The column containing the calculated Haversine distance.</p> Example <pre><code>&gt;&gt;&gt; df = spark.createDataFrame([(52.1552, 5.3876, 59.9111, 10.7503)], [\"lat1\", \"long1\", \"lat2\", \"long2\"])\n&gt;&gt;&gt; df.select(haversine_distance(\"lat1\", \"long1\", \"lat2\", \"long2\")).first()[0]\n923.8038\n</code></pre> Source code in <code>pysparky/functions/math_.py</code> <pre><code>@decorator.extension_enabler(Column)\ndef haversine_distance(\n    lat1: ColumnOrName,\n    long1: ColumnOrName,\n    lat2: ColumnOrName,\n    long2: ColumnOrName,\n) -&gt; Column:\n    \"\"\"\n    Calculates the Haversine distance between two sets of latitude and longitude coordinates.\n\n    Args:\n        lat1 (ColumnOrName): The column containing the latitude of the first coordinate.\n        long1 (ColumnOrName): The column containing the longitude of the first coordinate.\n        lat2 (ColumnOrName): The column containing the latitude of the second coordinate.\n        long2 (ColumnOrName): The column containing the longitude of the second coordinate.\n\n    Returns:\n        Column: The column containing the calculated Haversine distance.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; df = spark.createDataFrame([(52.1552, 5.3876, 59.9111, 10.7503)], [\"lat1\", \"long1\", \"lat2\", \"long2\"])\n        &gt;&gt;&gt; df.select(haversine_distance(\"lat1\", \"long1\", \"lat2\", \"long2\")).first()[0]\n        923.8038\n        ```\n    \"\"\"\n    # Convert latitude and longitude from degrees to radians\n    lat1_randians = F.radians(lat1)\n    long1_randians = F.radians(long1)\n    lat2_randians = F.radians(lat2)\n    long2_randians = F.radians(long2)\n\n    # Haversine formula\n    dlat = lat2_randians - lat1_randians\n    dlong = long2_randians - long1_randians\n    a = (\n        F.sin(dlat / 2) ** 2\n        + F.cos(lat1_randians) * F.cos(lat2_randians) * F.sin(dlong / 2) ** 2\n    )\n    c = 2 * F.atan2(F.sqrt(a), F.sqrt(1 - a))\n\n    # Radius of the Earth (in kilometers)\n    R = 6371.0\n\n    # Calculate the distance\n    distance = F.round(R * c, 4)\n\n    return distance.alias(\"haversine_distance\")\n</code></pre>"},{"location":"functions/math/#pysparky.functions.math_.sumif","title":"<code>sumif(condition, value=1, otherwise_value=0)</code>","text":"<p>Return a conditional sum using Spark expressions.</p> <p>Parameters:</p> Name Type Description Default <code>condition</code> <code>Column</code> <p>Boolean Spark expression to filter rows.</p> required <code>value</code> <code>Union[Column, int, float]</code> <p>Column or scalar to sum when condition is True.</p> <code>1</code> <code>otherwise_value</code> <code>Union[Column, int, float]</code> <p>Column or scalar to use when condition is False.</p> <code>0</code> <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>Spark aggregation expression that sums <code>value</code> when true,</p> <code>Column</code> <p>otherwise <code>otherwise_value</code>.</p> Example <pre><code>&gt;&gt;&gt; df = spark.createDataFrame([(\"A\", 10), (\"B\", 20), (\"A\", 30)], [\"category\", \"value\"])\n&gt;&gt;&gt; df.select(sumif(F.col(\"category\") == \"A\").alias(\"count_a\")).show()\n+-------+\n|count_a|\n+-------+\n|      2|\n+-------+\n\n&gt;&gt;&gt; df.select(sumif(F.col(\"category\") == \"A\", F.col(\"value\")).alias(\"sum_a\")).show()\n+-----+\n|sum_a|\n+-----+\n|   40|\n+-----+\n</code></pre> Source code in <code>pysparky/functions/math_.py</code> <pre><code>def sumif(\n    condition: Column,\n    value: Union[Column, int, float] = 1,\n    otherwise_value: Union[Column, int, float] = 0,\n) -&gt; Column:\n    \"\"\"Return a conditional sum using Spark expressions.\n\n    Args:\n        condition: Boolean Spark expression to filter rows.\n        value: Column or scalar to sum when condition is True.\n        otherwise_value: Column or scalar to use when condition is False.\n\n    Returns:\n        Column: Spark aggregation expression that sums `value` when true,\n        otherwise `otherwise_value`.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; df = spark.createDataFrame([(\"A\", 10), (\"B\", 20), (\"A\", 30)], [\"category\", \"value\"])\n        &gt;&gt;&gt; df.select(sumif(F.col(\"category\") == \"A\").alias(\"count_a\")).show()\n        +-------+\n        |count_a|\n        +-------+\n        |      2|\n        +-------+\n\n        &gt;&gt;&gt; df.select(sumif(F.col(\"category\") == \"A\", F.col(\"value\")).alias(\"sum_a\")).show()\n        +-----+\n        |sum_a|\n        +-----+\n        |   40|\n        +-----+\n        ```\n    \"\"\"\n    return F.sum(F.when(condition, value).otherwise(otherwise_value))\n</code></pre>"},{"location":"io/csv/","title":"Saving to single csv","text":""},{"location":"io/csv/#pysparky.io.csv.write_single_csv","title":"<code>write_single_csv(sdf, path, **csvKwargs)</code>","text":"<p>Writes a single CSV file from a Spark DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>The Spark DataFrame to write.</p> required <code>path</code> <code>str</code> <p>The destination path for the CSV file.</p> required <code>**csvKwargs</code> <p>Additional keyword arguments to pass to the DataFrame's write.csv method.</p> <code>{}</code> Example <pre><code>&gt;&gt;&gt; write_single_csv(spark.range(1), \"temp/file.csv\", header=True, mode='overwrite')\n</code></pre> Source code in <code>pysparky/io/csv.py</code> <pre><code>def write_single_csv(sdf: DataFrame, path: str, **csvKwargs):\n    \"\"\"\n    Writes a single CSV file from a Spark DataFrame.\n\n    Args:\n        sdf (DataFrame): The Spark DataFrame to write.\n        path (str): The destination path for the CSV file.\n        **csvKwargs: Additional keyword arguments to pass to the DataFrame's write.csv method.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; write_single_csv(spark.range(1), \"temp/file.csv\", header=True, mode='overwrite')\n        ```\n    \"\"\"\n    with tempfile.TemporaryDirectory() as temp_dir:\n        sdf.repartition(1).write.csv(temp_dir, **csvKwargs)\n        csv_file = glob.glob(os.path.join(temp_dir, \"*.csv\"))[0]\n        os.rename(csv_file, path)\n</code></pre>"},{"location":"transformations/dedup/","title":"Dedup","text":""},{"location":"transformations/dedup/#pysparky.transformations.dedup.get_latest_record_from_column","title":"<code>get_latest_record_from_column(sdf, window_partition_column_name, window_order_by_column_names, window_function=F.row_number)</code>","text":"<p>Fetches the most recent record from a DataFrame based on a specified column, allowing for custom sorting order.</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>The DataFrame to process.</p> required <code>window_partition_column_name</code> <code>str</code> <p>The column used to partition the DataFrame.</p> required <code>window_order_by_column_names</code> <code>str | list</code> <p>The column(s) used to sort the DataFrame.</p> required <code>window_function</code> <code>Column</code> <p>The window function for ranking records. Defaults to F.row_number.</p> <code>row_number</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A DataFrame with the most recent record for each partition.</p> Example <pre><code>&gt;&gt;&gt; data = [\n...     (\"A\", 1, \"2021-01-01\"),\n...     (\"A\", 2, \"2021-01-02\"),\n...     (\"B\", 1, \"2021-01-01\")\n... ]\n&gt;&gt;&gt; df = spark.createDataFrame(data, [\"id\", \"version\", \"date\"])\n&gt;&gt;&gt; result = get_latest_record_from_column(df, \"id\", \"date\")\n&gt;&gt;&gt; result.show()\n+---+-------+----------+\n| id|version|      date|\n+---+-------+----------+\n|  A|      2|2021-01-02|\n|  B|      1|2021-01-01|\n+---+-------+----------+\n</code></pre> Source code in <code>pysparky/transformations/dedup.py</code> <pre><code>@decorator.extension_enabler(DataFrame)\ndef get_latest_record_from_column(\n    sdf: DataFrame,\n    window_partition_column_name: str,\n    window_order_by_column_names: str | list[str],\n    window_function: Callable = F.row_number,\n) -&gt; DataFrame:\n    \"\"\"\n    Fetches the most recent record from a DataFrame based on a specified column, allowing for custom sorting order.\n\n    Parameters:\n        sdf (DataFrame): The DataFrame to process.\n        window_partition_column_name (str): The column used to partition the DataFrame.\n        window_order_by_column_names (str | list): The column(s) used to sort the DataFrame.\n        window_function (Column, optional): The window function for ranking records. Defaults to F.row_number.\n\n    Returns:\n        DataFrame: A DataFrame with the most recent record for each partition.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; data = [\n        ...     (\"A\", 1, \"2021-01-01\"),\n        ...     (\"A\", 2, \"2021-01-02\"),\n        ...     (\"B\", 1, \"2021-01-01\")\n        ... ]\n        &gt;&gt;&gt; df = spark.createDataFrame(data, [\"id\", \"version\", \"date\"])\n        &gt;&gt;&gt; result = get_latest_record_from_column(df, \"id\", \"date\")\n        &gt;&gt;&gt; result.show()\n        +---+-------+----------+\n        | id|version|      date|\n        +---+-------+----------+\n        |  A|      2|2021-01-02|\n        |  B|      1|2021-01-01|\n        +---+-------+----------+\n        ```\n    \"\"\"\n    window_order_by_column_names = enabler.ensure_list(window_order_by_column_names)\n\n    return (\n        sdf.withColumn(\n            \"temp\",\n            window_function().over(\n                Window.partitionBy(window_partition_column_name).orderBy(\n                    *window_order_by_column_names\n                )\n            ),\n        )\n        .filter(F.col(\"temp\") == 1)\n        .drop(\"temp\")\n    )\n</code></pre>"},{"location":"transformations/dedup/#pysparky.transformations.dedup.get_only_duplicate_record","title":"<code>get_only_duplicate_record(sdf, column_name)</code>","text":"<p>Retrieves only the duplicate records from the input DataFrame based on the specified column.</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>The input Spark DataFrame.</p> required <code>column_name</code> <code>str</code> <p>The column name to check for duplicates.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A DataFrame containing only the duplicate records.</p> Example <pre><code>&gt;&gt;&gt; data = [(1, \"A\"), (2, \"B\"), (3, \"C\"), (1, \"A\"), (4, \"D\"), (2, \"B\")]\n&gt;&gt;&gt; sdf = spark.createDataFrame(data, [\"id\", \"value\"])\n&gt;&gt;&gt; duplicate_records = get_only_duplicate_record(sdf, \"id\")\n&gt;&gt;&gt; duplicate_records.show()\n+---+-----+\n| id|value|\n+---+-----+\n|  1|    A|\n|  1|    A|\n|  2|    B|\n|  2|    B|\n+---+-----+\n</code></pre> Source code in <code>pysparky/transformations/dedup.py</code> <pre><code>def get_only_duplicate_record(sdf: DataFrame, column_name: str) -&gt; DataFrame:\n    \"\"\"\n    Retrieves only the duplicate records from the input DataFrame\n    based on the specified column.\n\n    Args:\n        sdf (DataFrame): The input Spark DataFrame.\n        column_name (str): The column name to check for duplicates.\n\n    Returns:\n        DataFrame: A DataFrame containing only the duplicate records.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; data = [(1, \"A\"), (2, \"B\"), (3, \"C\"), (1, \"A\"), (4, \"D\"), (2, \"B\")]\n        &gt;&gt;&gt; sdf = spark.createDataFrame(data, [\"id\", \"value\"])\n        &gt;&gt;&gt; duplicate_records = get_only_duplicate_record(sdf, \"id\")\n        &gt;&gt;&gt; duplicate_records.show()\n        +---+-----+\n        | id|value|\n        +---+-----+\n        |  1|    A|\n        |  1|    A|\n        |  2|    B|\n        |  2|    B|\n        +---+-----+\n        ```\n    \"\"\"\n    _, duplicate_records_sdf = quarantine_duplicate_record(sdf, column_name)\n    return duplicate_records_sdf\n</code></pre>"},{"location":"transformations/dedup/#pysparky.transformations.dedup.get_only_unique_record","title":"<code>get_only_unique_record(sdf, column_name)</code>","text":"<p>Retrieves only the unique records from the input DataFrame based on the specified column.</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>The input Spark DataFrame.</p> required <code>column_name</code> <code>str</code> <p>The column name to check for duplicates.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A DataFrame containing only the unique records.</p> Example <pre><code>&gt;&gt;&gt; data = [(1, \"A\"), (2, \"B\"), (3, \"C\"), (1, \"A\"), (4, \"D\"), (2, \"B\")]\n&gt;&gt;&gt; sdf = spark.createDataFrame(data, [\"id\", \"value\"])\n&gt;&gt;&gt; unique_records = get_only_unique_record(sdf, \"id\")\n&gt;&gt;&gt; unique_records.show()\n+---+-----+\n| id|value|\n+---+-----+\n|  3|    C|\n|  4|    D|\n+---+-----+\n</code></pre> Source code in <code>pysparky/transformations/dedup.py</code> <pre><code>def get_only_unique_record(sdf: DataFrame, column_name: str) -&gt; DataFrame:\n    \"\"\"\n    Retrieves only the unique records from the input DataFrame\n    based on the specified column.\n\n    Args:\n        sdf (DataFrame): The input Spark DataFrame.\n        column_name (str): The column name to check for duplicates.\n\n    Returns:\n        DataFrame: A DataFrame containing only the unique records.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; data = [(1, \"A\"), (2, \"B\"), (3, \"C\"), (1, \"A\"), (4, \"D\"), (2, \"B\")]\n        &gt;&gt;&gt; sdf = spark.createDataFrame(data, [\"id\", \"value\"])\n        &gt;&gt;&gt; unique_records = get_only_unique_record(sdf, \"id\")\n        &gt;&gt;&gt; unique_records.show()\n        +---+-----+\n        | id|value|\n        +---+-----+\n        |  3|    C|\n        |  4|    D|\n        +---+-----+\n        ```\n    \"\"\"\n    unique_records_sdf, _ = quarantine_duplicate_record(sdf, column_name)\n    return unique_records_sdf\n</code></pre>"},{"location":"transformations/dedup/#pysparky.transformations.dedup.quarantine_duplicate_record","title":"<code>quarantine_duplicate_record(sdf, column_name)</code>","text":"<p>Splits the input DataFrame into two DataFrames: one containing unique records based on the specified column, and the other containing duplicate records.</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>The input Spark DataFrame.</p> required <code>column_name</code> <code>str</code> <p>The column name to check for duplicates.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>tuple[DataFrame, DataFrame]: A tuple containing two DataFrames. The first DataFrame</p> <code>DataFrame</code> <p>contains unique records, and the second DataFrame contains duplicate records.</p> Example <pre><code>&gt;&gt;&gt; data = [(1, \"A\"), (2, \"B\"), (3, \"C\"), (1, \"A\"), (4, \"D\"), (2, \"B\")]\n&gt;&gt;&gt; sdf = spark.createDataFrame(data, [\"id\", \"value\"])\n&gt;&gt;&gt; unique_records, duplicate_records = quarantine_duplicate_record(sdf, \"id\")\n&gt;&gt;&gt; unique_records.show()\n+---+-----+\n| id|value|\n+---+-----+\n|  3|    C|\n|  4|    D|\n+---+-----+\n&gt;&gt;&gt; duplicate_records.show()\n+---+-----+\n| id|value|\n+---+-----+\n|  1|    A|\n|  1|    A|\n|  2|    B|\n|  2|    B|\n+---+-----+\n</code></pre> Source code in <code>pysparky/transformations/dedup.py</code> <pre><code>def quarantine_duplicate_record(\n    sdf: DataFrame, column_name: str\n) -&gt; tuple[DataFrame, DataFrame]:\n    \"\"\"\n    Splits the input DataFrame into two DataFrames: one containing unique records\n    based on the specified column, and the other containing duplicate records.\n\n    Args:\n        sdf (DataFrame): The input Spark DataFrame.\n        column_name (str): The column name to check for duplicates.\n\n    Returns:\n        tuple[DataFrame, DataFrame]: A tuple containing two DataFrames. The first DataFrame\n        contains unique records, and the second DataFrame contains duplicate records.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; data = [(1, \"A\"), (2, \"B\"), (3, \"C\"), (1, \"A\"), (4, \"D\"), (2, \"B\")]\n        &gt;&gt;&gt; sdf = spark.createDataFrame(data, [\"id\", \"value\"])\n        &gt;&gt;&gt; unique_records, duplicate_records = quarantine_duplicate_record(sdf, \"id\")\n        &gt;&gt;&gt; unique_records.show()\n        +---+-----+\n        | id|value|\n        +---+-----+\n        |  3|    C|\n        |  4|    D|\n        +---+-----+\n        &gt;&gt;&gt; duplicate_records.show()\n        +---+-----+\n        | id|value|\n        +---+-----+\n        |  1|    A|\n        |  1|    A|\n        |  2|    B|\n        |  2|    B|\n        +---+-----+\n        ```\n    \"\"\"\n    window_spec = Window.partitionBy(column_name)\n    with_count_sdf = sdf.withColumn(\"count\", F.count(column_name).over(window_spec))\n    unique_records_sdf = with_count_sdf.filter(F.col(\"count\") == 1).drop(\"count\")\n    duplicate_records_sdf = with_count_sdf.filter(F.col(\"count\") &gt; 1).drop(\"count\")\n\n    return unique_records_sdf, duplicate_records_sdf\n</code></pre>"},{"location":"transformations/general/","title":"General","text":""},{"location":"transformations/general/#pysparky.transformations.general.agg_apply","title":"<code>agg_apply(df, agg_exprs)</code>","text":"<p>Apply aggregation expressions and return a DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Spark DataFrame.</p> required <code>agg_exprs</code> <code>Dict[str, Column]</code> <p>A dictionary where keys are the aliases for the aggregated columns                            and values are the Spark aggregation expressions (Columns).</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A DataFrame containing the aggregated results.</p> Example <pre><code>&gt;&gt;&gt; df = spark.createDataFrame([(1, \"A\"), (2, \"A\"), (3, \"B\")], [\"value\", \"category\"])\n&gt;&gt;&gt; agg_exprs = {\n...     \"total_value\": F.sum(\"value\"),\n...     \"max_value\": F.max(\"value\"),\n...     \"count\": F.count(\"*\")\n... }\n&gt;&gt;&gt; result = agg_apply(df, agg_exprs)\n&gt;&gt;&gt; result.show()\n+-----------+---------+-----+\n|total_value|max_value|count|\n+-----------+---------+-----+\n|          6|        3|    3|\n+-----------+---------+-----+\n</code></pre> Source code in <code>pysparky/transformations/general.py</code> <pre><code>def agg_apply(df: DataFrame, agg_exprs: Dict[str, Column]) -&gt; DataFrame:\n    \"\"\"Apply aggregation expressions and return a DataFrame.\n\n    Args:\n        df (DataFrame): The input Spark DataFrame.\n        agg_exprs (Dict[str, Column]): A dictionary where keys are the aliases for the aggregated columns\n                                       and values are the Spark aggregation expressions (Columns).\n\n    Returns:\n        DataFrame: A DataFrame containing the aggregated results.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; df = spark.createDataFrame([(1, \"A\"), (2, \"A\"), (3, \"B\")], [\"value\", \"category\"])\n        &gt;&gt;&gt; agg_exprs = {\n        ...     \"total_value\": F.sum(\"value\"),\n        ...     \"max_value\": F.max(\"value\"),\n        ...     \"count\": F.count(\"*\")\n        ... }\n        &gt;&gt;&gt; result = agg_apply(df, agg_exprs)\n        &gt;&gt;&gt; result.show()\n        +-----------+---------+-----+\n        |total_value|max_value|count|\n        +-----------+---------+-----+\n        |          6|        3|    3|\n        +-----------+---------+-----+\n        ```\n    \"\"\"\n    return df.agg(*[expr.alias(name) for name, expr in agg_exprs.items()])\n</code></pre>"},{"location":"transformations/general/#pysparky.transformations.general.apply_cols","title":"<code>apply_cols(sdf, col_func, cols=None, **kwargs)</code>","text":"<p>Apply a function to specified columns of a Spark DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>The input Spark DataFrame.</p> required <code>col_func</code> <code>callable</code> <p>The function to apply to each column.</p> required <code>cols</code> <code>list[str]</code> <p>List of column names to apply the function to.                         If None, applies to all columns. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A new Spark DataFrame with the function applied to the specified columns.</p> Source code in <code>pysparky/transformations/general.py</code> <pre><code>@decorator.extension_enabler(DataFrame)\ndef apply_cols(\n    sdf: DataFrame, col_func: Callable, cols: list[str] | None = None, **kwargs\n) -&gt; DataFrame:\n    \"\"\"\n    Apply a function to specified columns of a Spark DataFrame.\n\n    Parameters:\n        sdf (DataFrame): The input Spark DataFrame.\n        col_func (callable): The function to apply to each column.\n        cols (list[str], optional): List of column names to apply the function to.\n                                    If None, applies to all columns. Defaults to None.\n\n    Returns:\n        DataFrame: A new Spark DataFrame with the function applied to the specified columns.\n    \"\"\"\n    if cols is None:\n        cols = sdf.columns\n    return sdf.withColumns(\n        {col_name: col_func(col_name, **kwargs) for col_name in cols}\n    )\n</code></pre>"},{"location":"transformations/general/#pysparky.transformations.general.distinct_value_counts_map","title":"<code>distinct_value_counts_map(sdf, column_name)</code>","text":"<p>Get distinct values from a DataFrame column as a map with their counts.</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>The input Spark DataFrame.</p> required <code>column_name</code> <code>str</code> <p>The name of the column to process.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A DataFrame containing a single column with a map of distinct values and their counts.</p> Example <pre><code>&gt;&gt;&gt; data = [(\"Alice\",), (\"Bob\",), (\"Alice\",), (\"Eve\",), (None,)]\n&gt;&gt;&gt; sdf = spark.createDataFrame(data, [\"name\"])\n&gt;&gt;&gt; result = distinct_value_counts_map(sdf, \"name\")\n&gt;&gt;&gt; result.show(truncate=False)\n+-------------------------------------------+\n|name_map                                   |\n+-------------------------------------------+\n|{Alice -&gt; 2, Bob -&gt; 1, Eve -&gt; 1, NONE -&gt; 1}|\n+-------------------------------------------+\n</code></pre> Source code in <code>pysparky/transformations/general.py</code> <pre><code>@decorator.extension_enabler(DataFrame)\ndef distinct_value_counts_map(sdf: DataFrame, column_name: str) -&gt; DataFrame:\n    \"\"\"\n    Get distinct values from a DataFrame column as a map with their counts.\n\n    Args:\n        sdf (DataFrame): The input Spark DataFrame.\n        column_name (str): The name of the column to process.\n\n    Returns:\n        DataFrame: A DataFrame containing a single column with a map of distinct values and their counts.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; data = [(\"Alice\",), (\"Bob\",), (\"Alice\",), (\"Eve\",), (None,)]\n        &gt;&gt;&gt; sdf = spark.createDataFrame(data, [\"name\"])\n        &gt;&gt;&gt; result = distinct_value_counts_map(sdf, \"name\")\n        &gt;&gt;&gt; result.show(truncate=False)\n        +-------------------------------------------+\n        |name_map                                   |\n        +-------------------------------------------+\n        |{Alice -&gt; 2, Bob -&gt; 1, Eve -&gt; 1, NONE -&gt; 1}|\n        +-------------------------------------------+\n        ```\n    \"\"\"\n    return (\n        sdf.select(column_name)\n        .na.fill(\"NONE\")\n        .groupBy(column_name)\n        .count()\n        .select(\n            F.map_from_entries(F.collect_list(F.struct(column_name, \"count\"))).alias(\n                f\"{column_name}_map\"\n            )\n        )\n    )\n</code></pre>"},{"location":"transformations/general/#pysparky.transformations.general.execute_transformation_blueprint","title":"<code>execute_transformation_blueprint(sdf, blueprint)</code>","text":"<p>Executes a transformation blueprint on a Spark DataFrame.</p> <p>The transformation blueprint is a dictionary where keys are column names and values are the corresponding transformations to apply. The function applies each transformation in the order specified by the blueprint and returns the resulting DataFrame with the transformed columns.</p> <p>A transformation_blueprint is a dictionary that the key: new column name value: Column function</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>The input DataFrame to be transformed.</p> required <code>blueprint</code> <code>Dict[str, Column]</code> <p>A dictionary of column names as keys and transformation functions as values.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>The resulting DataFrame with the transformed columns.</p> Example <pre><code>&gt;&gt;&gt; df = spark.createDataFrame([(1,), (2,)], [\"value\"])\n&gt;&gt;&gt; blueprint = {\"value_plus_1\": F.col(\"value\") + 1}\n&gt;&gt;&gt; df.transform(execute_transformation_blueprint, blueprint).show()\n+------------+\n|value_plus_1|\n+------------+\n|           2|\n|           3|\n+------------+\n</code></pre> Source code in <code>pysparky/transformations/general.py</code> <pre><code>def execute_transformation_blueprint(\n    sdf: DataFrame, blueprint: dict[str, Column]\n) -&gt; DataFrame:\n    \"\"\"\n    Executes a transformation blueprint on a Spark DataFrame.\n\n    The transformation blueprint is a dictionary where keys are column names\n    and values are the corresponding transformations to apply. The function\n    applies each transformation in the order specified by the blueprint and\n    returns the resulting DataFrame with the transformed columns.\n\n    A transformation_blueprint is a dictionary that the\n    key: new column name\n    value: Column function\n\n    Args:\n        sdf (DataFrame): The input DataFrame to be transformed.\n        blueprint (Dict[str, Column]): A dictionary of column names as keys and\n            transformation functions as values.\n\n    Returns:\n        DataFrame: The resulting DataFrame with the transformed columns.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; df = spark.createDataFrame([(1,), (2,)], [\"value\"])\n        &gt;&gt;&gt; blueprint = {\"value_plus_1\": F.col(\"value\") + 1}\n        &gt;&gt;&gt; df.transform(execute_transformation_blueprint, blueprint).show()\n        +------------+\n        |value_plus_1|\n        +------------+\n        |           2|\n        |           3|\n        +------------+\n        ```\n    \"\"\"\n    return sdf.select(\n        [\n            column_processing.alias(new_column_name)\n            for new_column_name, column_processing in blueprint.items()\n        ]\n    )\n</code></pre>"},{"location":"transformations/general/#pysparky.transformations.general.filters","title":"<code>filters(sdf, conditions, operator_='and')</code>","text":"<p>Apply multiple filter conditions to a Spark DataFrame.</p> <p>This function takes a Spark DataFrame, a list of conditions, and an optional operator. It returns a new DataFrame with all conditions applied using the specified operator.</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>The input Spark DataFrame to be filtered.</p> required <code>conditions</code> <code>list[Column]</code> <p>A list of Column expressions representing the filter conditions.</p> required <code>operator_</code> <code>Callable</code> <p>The operator to use for combining conditions. Defaults to <code>and_</code>. Valid options are <code>and_</code> and <code>or_</code>.</p> <code>'and'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.DataFrame: A new DataFrame with all filter conditions applied.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an unsupported operator is provided.</p> Example <pre><code>&gt;&gt;&gt; from pyspark.sql.functions import col\n&gt;&gt;&gt; df = spark.createDataFrame([(1, 'a'), (2, 'b'), (3, 'c')], ['id', 'letter'])\n&gt;&gt;&gt; conditions = [col('id') &gt; 1, col('letter').isin(['b', 'c'])]\n\n# Filter using AND (default behavior)\n&gt;&gt;&gt; filtered_df_and = filters(df, conditions)\n&gt;&gt;&gt; filtered_df_and.show()\n+---+------+\n| id|letter|\n+---+------+\n|  2|     b|\n|  3|     c|\n+---+------+\n\n# Filter using OR\n&gt;&gt;&gt; filtered_df_or = filters(df, conditions, or_)\n&gt;&gt;&gt; filtered_df_or.show()\n+---+------+\n| id|letter|\n+---+------+\n|  2|     b|\n|  3|     c|\n|  1|     a|\n+---+------+\n</code></pre> Source code in <code>pysparky/transformations/general.py</code> <pre><code>@decorator.extension_enabler(DataFrame)\ndef filters(\n    sdf: DataFrame, conditions: list[Column], operator_: str = \"and\"\n) -&gt; DataFrame:\n    \"\"\"\n    Apply multiple filter conditions to a Spark DataFrame.\n\n    This function takes a Spark DataFrame, a list of conditions, and an optional\n    operator. It returns a new DataFrame with all conditions applied using the\n    specified operator.\n\n    Args:\n        sdf (pyspark.sql.DataFrame): The input Spark DataFrame to be filtered.\n        conditions (list[pyspark.sql.Column]): A list of Column expressions\n            representing the filter conditions.\n        operator_ (Callable, optional): The operator to use for combining\n            conditions. Defaults to `and_`. Valid options are `and_` and `or_`.\n\n    Returns:\n        pyspark.sql.DataFrame: A new DataFrame with all filter conditions applied.\n\n    Raises:\n        ValueError: If an unsupported operator is provided.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; from pyspark.sql.functions import col\n        &gt;&gt;&gt; df = spark.createDataFrame([(1, 'a'), (2, 'b'), (3, 'c')], ['id', 'letter'])\n        &gt;&gt;&gt; conditions = [col('id') &gt; 1, col('letter').isin(['b', 'c'])]\n\n        # Filter using AND (default behavior)\n        &gt;&gt;&gt; filtered_df_and = filters(df, conditions)\n        &gt;&gt;&gt; filtered_df_and.show()\n        +---+------+\n        | id|letter|\n        +---+------+\n        |  2|     b|\n        |  3|     c|\n        +---+------+\n\n        # Filter using OR\n        &gt;&gt;&gt; filtered_df_or = filters(df, conditions, or_)\n        &gt;&gt;&gt; filtered_df_or.show()\n        +---+------+\n        | id|letter|\n        +---+------+\n        |  2|     b|\n        |  3|     c|\n        |  1|     a|\n        +---+------+\n        ```\n    \"\"\"\n    match operator_:\n        case \"and\":\n            operator_callable = and_\n        case \"or\":\n            operator_callable = or_\n        case _:\n            raise ValueError(\n                f\"Unsupported operator: {operator_}. Valid options are 'and' and 'or'.\"\n            )\n\n    default_value = F.lit(True) if operator_callable == and_ else F.lit(False)\n    return sdf.filter(reduce(operator_callable, conditions, default_value))\n</code></pre>"},{"location":"transformations/general/#pysparky.transformations.general.get_unique_values","title":"<code>get_unique_values(df1, df2, column_name)</code>","text":"<p>Unions two DataFrames and returns a DataFrame with unique values.</p> <p>Parameters:</p> Name Type Description Default <code>df1</code> <code>DataFrame</code> <p>First DataFrame.</p> required <code>df2</code> <code>DataFrame</code> <p>Second DataFrame.</p> required <code>column_name</code> <code>str</code> <p>The column name containing the values.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A DataFrame with unique values.</p> Example <pre><code>&gt;&gt;&gt; df1 = spark.createDataFrame([(1,), (2,), (3,)], [\"value\"])\n&gt;&gt;&gt; df2 = spark.createDataFrame([(3,), (4,), (5,)], [\"value\"])\n&gt;&gt;&gt; unique_values = get_unique_values(df1, df2, \"value\")\n&gt;&gt;&gt; unique_values.sort(\"value\").show()\n+-----+\n|value|\n+-----+\n|    1|\n|    2|\n|    3|\n|    4|\n|    5|\n+-----+\n</code></pre> Source code in <code>pysparky/transformations/general.py</code> <pre><code>def get_unique_values(df1: DataFrame, df2: DataFrame, column_name: str) -&gt; DataFrame:\n    \"\"\"Unions two DataFrames and returns a DataFrame with unique values.\n\n    Args:\n        df1 (DataFrame): First DataFrame.\n        df2 (DataFrame): Second DataFrame.\n        column_name (str): The column name containing the values.\n\n    Returns:\n        DataFrame: A DataFrame with unique values.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; df1 = spark.createDataFrame([(1,), (2,), (3,)], [\"value\"])\n        &gt;&gt;&gt; df2 = spark.createDataFrame([(3,), (4,), (5,)], [\"value\"])\n        &gt;&gt;&gt; unique_values = get_unique_values(df1, df2, \"value\")\n        &gt;&gt;&gt; unique_values.sort(\"value\").show()\n        +-----+\n        |value|\n        +-----+\n        |    1|\n        |    2|\n        |    3|\n        |    4|\n        |    5|\n        +-----+\n        ```\n    \"\"\"\n    # Union the DataFrames\n    union_df = df1.select(column_name).union(df2.select(column_name))\n\n    # Perform distinct to get unique values\n    unique_values_df = union_df.distinct()\n\n    return unique_values_df\n</code></pre>"},{"location":"transformations/general/#pysparky.transformations.general.set_columns_to_null_based_on_condition","title":"<code>set_columns_to_null_based_on_condition(df, condition_column, condition_value, target_columns)</code>","text":"<p>Sets specified columns to null based on a condition in the given DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame.</p> required <code>condition_column</code> <code>str</code> <p>The name of the column containing the condition value.</p> required <code>condition_value</code> <code>str</code> <p>The value indicating the condition to set columns to null.</p> required <code>target_columns</code> <code>Tuple[str]</code> <p>The tuple of columns to be set as null if the condition value is found.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>The updated DataFrame with specified columns set to null based on the condition.</p> Example <pre><code>&gt;&gt;&gt; data = [\n...     (1, 0, 0, 0),\n...     (2, 0, 1, 0),\n...     (3, 1, 1, 1),\n...     (4, 1, 0, 1),\n...     (5, 0, 0, 0),\n... ]\n&gt;&gt;&gt; columns = [\"ID\", \"Dummy1\", \"Dummy2\", \"Dummy3\"]\n&gt;&gt;&gt; df = spark.createDataFrame(data, columns)\n&gt;&gt;&gt; condition_column = \"Dummy1\"\n&gt;&gt;&gt; condition_value = 1\n&gt;&gt;&gt; target_columns = (\"Dummy2\", \"Dummy3\")\n&gt;&gt;&gt; result_df = set_columns_to_null_based_on_condition(df, condition_column, condition_value, target_columns)\n&gt;&gt;&gt; result_df.show()\n+---+------+------+------+\n| ID|Dummy1|Dummy2|Dummy3|\n+---+------+------+------+\n|  1|     0|     0|     0|\n|  2|     0|     1|     0|\n|  3|     1|  null|  null|\n|  4|     1|  null|  null|\n|  5|     0|     0|     0|\n+---+------+------+------+\n</code></pre> Source code in <code>pysparky/transformations/general.py</code> <pre><code>def set_columns_to_null_based_on_condition(\n    df: DataFrame,\n    condition_column: str,\n    condition_value: str,\n    target_columns: tuple[str],\n) -&gt; DataFrame:\n    \"\"\"\n    Sets specified columns to null based on a condition in the given DataFrame.\n\n    Args:\n        df (DataFrame): The input DataFrame.\n        condition_column (str): The name of the column containing the condition value.\n        condition_value (str): The value indicating the condition to set columns to null.\n        target_columns (Tuple[str]): The tuple of columns to be set as null if the condition value is found.\n\n    Returns:\n        DataFrame: The updated DataFrame with specified columns set to null based on the condition.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; data = [\n        ...     (1, 0, 0, 0),\n        ...     (2, 0, 1, 0),\n        ...     (3, 1, 1, 1),\n        ...     (4, 1, 0, 1),\n        ...     (5, 0, 0, 0),\n        ... ]\n        &gt;&gt;&gt; columns = [\"ID\", \"Dummy1\", \"Dummy2\", \"Dummy3\"]\n        &gt;&gt;&gt; df = spark.createDataFrame(data, columns)\n        &gt;&gt;&gt; condition_column = \"Dummy1\"\n        &gt;&gt;&gt; condition_value = 1\n        &gt;&gt;&gt; target_columns = (\"Dummy2\", \"Dummy3\")\n        &gt;&gt;&gt; result_df = set_columns_to_null_based_on_condition(df, condition_column, condition_value, target_columns)\n        &gt;&gt;&gt; result_df.show()\n        +---+------+------+------+\n        | ID|Dummy1|Dummy2|Dummy3|\n        +---+------+------+------+\n        |  1|     0|     0|     0|\n        |  2|     0|     1|     0|\n        |  3|     1|  null|  null|\n        |  4|     1|  null|  null|\n        |  5|     0|     0|     0|\n        +---+------+------+------+\n        ```\n    \"\"\"\n    return df.withColumns(\n        {\n            col: F.when(\n                F.col(condition_column) == condition_value, F.lit(None)\n            ).otherwise(F.col(col))\n            for col in target_columns\n        }\n    )\n</code></pre>"},{"location":"transformations/general/#pysparky.transformations.general.transforms","title":"<code>transforms(sdf, transformations)</code>","text":"<p>Apply a series of transformations to a Spark DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>The input Spark DataFrame to be transformed.</p> required <code>transformations</code> <code>list</code> <p>A list of transformations, where each transformation is a tuple             containing a function and a dictionary of keyword arguments to apply the function to.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>The transformed Spark DataFrame.</p> Example <pre><code>&gt;&gt;&gt; def add_one(sdf, col_name):\n...     return sdf.withColumn(col_name, F.col(col_name) + 1)\n&gt;&gt;&gt; df = spark.createDataFrame([(1,), (2,)], [\"value\"])\n&gt;&gt;&gt; transformations = [(add_one, {\"col_name\": \"value\"})]\n&gt;&gt;&gt; df.transform(transforms, transformations).show()\n+-----+\n|value|\n+-----+\n|    2|\n|    3|\n+-----+\n</code></pre> Source code in <code>pysparky/transformations/general.py</code> <pre><code>@decorator.extension_enabler(DataFrame)\ndef transforms(\n    sdf: DataFrame, transformations: list[tuple[Callable, dict]]\n) -&gt; DataFrame:\n    \"\"\"\n    Apply a series of transformations to a Spark DataFrame.\n\n    Parameters:\n        sdf (DataFrame): The input Spark DataFrame to be transformed.\n        transformations (list): A list of transformations, where each transformation is a tuple\n                        containing a function and a dictionary of keyword arguments to apply the function to.\n\n    Returns:\n        DataFrame: The transformed Spark DataFrame.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; def add_one(sdf, col_name):\n        ...     return sdf.withColumn(col_name, F.col(col_name) + 1)\n        &gt;&gt;&gt; df = spark.createDataFrame([(1,), (2,)], [\"value\"])\n        &gt;&gt;&gt; transformations = [(add_one, {\"col_name\": \"value\"})]\n        &gt;&gt;&gt; df.transform(transforms, transformations).show()\n        +-----+\n        |value|\n        +-----+\n        |    2|\n        |    3|\n        +-----+\n        ```\n    \"\"\"\n    for transformation_funcs, kwarg in transformations:\n        assert callable(transformation_funcs), \"transformation_funcs must be callable\"\n        assert isinstance(kwarg, dict), \"kwarg must be a dictionary\"\n        sdf = sdf.transform(transformation_funcs, **kwarg)\n    return sdf\n</code></pre>"}]}